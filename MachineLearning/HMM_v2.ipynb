{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kunqi Jiang   \n",
    "Created by Sep 23 2018; 2:13 AM \n",
    "\n",
    "In this notebook, I record the knowledge of hidden markov model(HMM), and apply to Part-Of-Speech Tagging using the Penn-Treebank dataset.  \n",
    "\n",
    "The three main problems in HMM to solve:    \n",
    "\n",
    "(1) compute the joint probability $p(O|\\lambda)$ of observed sequence $O$ given parameter $\\lambda=(A,B,\\pi)$.    \n",
    "\n",
    "(2) Given parameter $\\lambda$ and sequence $O$, compute max likelihood sequence of hidden states $P(I|O,\\lambda)$.\n",
    "\n",
    "(3) Learning parameters $\\lambda$ by EM algorithm maximizing $p(O|\\lambda)$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [1,2,2,2,1]\n",
    "states = ['h','m','l','n']\n",
    "obs_set = [1,2]\n",
    "\n",
    "# A\n",
    "trans_prob = {'h':{'h':0.5,'m':0.2,'l':0.2,'n':0.1},\n",
    "              'm':{'h':0.1,'m':0.5,'l':0.2,'n':0.2},\n",
    "              'l':{'h':0.2,'m':0.3,'l':0.1,'n':0.4},\n",
    "              'n':{'h':0.3,'m':0.1,'l':0.4,'n':0.2}}\n",
    "# B\n",
    "emm_prob = {'h':{1:0.7,2:0.3},\n",
    "            'm':{1:0.6,2:0.4},\n",
    "            'l':{1:0.1,2:0.9},\n",
    "            'n':{1:0.5,2:0.5}}\n",
    "# Pi\n",
    "init_prob = {'h':0.4,'m':0.1,'l':0.3,'n':0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "Start from the first state:   \n",
    "\n",
    "$$\\alpha(h_1) = \\pi(h_1)p(v_1|h_1)$$\n",
    "\n",
    "then move forward recursively by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha(h_t) &=& p(h_t,v_{1:t}) \\\\\n",
    "&=& \\sum_{h_{t-1}}p(v_t,h_t,h_{t-1},v_{1:t-1}) \\\\\n",
    "&=& \\sum_{h_{t-1}}p(v_t|h_t,h_{t-1},v_{1:t-1})p(h_t|h_{t-1},v_{1:t-1}) p(h_{t-1},v_{1:t-1}) \\\\\n",
    "&=& p(v_t|h_t) \\sum_{h_{t-1}}p(h_t|h_{t-1})p(h_{t-1},v_{1:t-1}) \\\\\n",
    "&=& p(v_t|h_t) \\sum_{h_{t-1}}p(h_t|h_{t-1})\\alpha(h_{t-1})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(observations,trans_prob,emm_prob,init_prob,t,h_t):\n",
    "    cur_alpha = {}\n",
    "    first_obs = observations[0]\n",
    "    states = init_prob.keys()\n",
    "    for state in states:\n",
    "        cur_alpha[state] = init_prob[state] * emm_prob[state][first_obs]\n",
    "    pre_alpha = dict(cur_alpha)\n",
    "    for obs in observations[1:t]:\n",
    "        for state in states:\n",
    "            cur_alpha[state] = emm_prob[state][obs] * \\\n",
    "                               sum((trans_prob[pre_st][state] * \\\n",
    "                               pre_alpha[pre_st]) \n",
    "                                 for pre_st in states)\n",
    "        pre_alpha = dict(cur_alpha)\n",
    "        \n",
    "    return cur_alpha[h_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010133999399999998"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(observations,trans_prob,emm_prob,init_prob,5,'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Likelihood ----- first problem\n",
    "$p(v_{1:T}) = \\sum_{h_T}p(h_T,v_{1:T}) = \\sum_{h_T}\\alpha(h_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(observations,trans_prob,emm_prob,init_prob,t):\n",
    "    states = init_prob.keys()\n",
    "    return sum(forward(observations,trans_prob,emm_prob,init_prob,t,h_t)\n",
    "               for h_t in states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0288189951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(observations,trans_prob,emm_prob,init_prob,len(observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "\n",
    "Start from the last state $$\\beta(h_T) = 1$$\n",
    "\n",
    "then move backward recursively by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta(h_t) &=& p(v_{t+1:T}|h_t) \\\\\n",
    "&=& \\sum_{h_{t+1}}p(v_{{t+2}:T},v_{t+1},h_{t+1}|h_{t}) \\\\\n",
    "&=& \\sum_{h_{t+1}}p(v_{t+2:T}|v_{t+1},h_{t+1},h_{t})p(v_{t+1}|h_{t+1},h_{t})p(h_{t+1}|h_{t}) \\\\\n",
    "&=& \\sum_{h_{t+1}}p(v_{t+2:T}|h_{t+1})p(v_{t+1}|h_{t+1})p(h_{t+1}|h_{t}) \\\\\n",
    "&=& \\sum_{h_{t+1}}p(v_{t+1}|h_{t+1})p(h_{t+1}|h_{t})\\beta(h_{t+1})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(observations,trans_prob,emm_prob,init_prob,t,h_t):\n",
    "    cur_beta = {}\n",
    "    states = init_prob.keys()\n",
    "    for state in states:\n",
    "        cur_beta[state] = 1\n",
    "    pre_beta = dict(cur_beta)\n",
    "    for obs in reversed(observations[t:]):\n",
    "        for state in states:\n",
    "            cur_beta[state] = sum(emm_prob[next_state][obs] \\\n",
    "                                 *trans_prob[state][next_state] \\\n",
    "                                 *pre_beta[next_state] \n",
    "                                  for next_state in states)\n",
    "        pre_beta = dict(cur_beta)\n",
    "    return cur_beta[h_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11022500000000002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward(observations,trans_prob,emm_prob,init_prob,2,'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\\begin{eqnarray}\n",
    "p(h_t,v_{1:T}) &=& p(v_{t+1:T},h_t,v_{1:t}) \\\\\n",
    "&=& p(v_{t+1:T}|h_t,v_{1:t})p(h_t,v_{1:t}) \\\\\n",
    "&=& p(h_1,v_{1:t}) p(v_{t+1:T}|h_t) \\\\\n",
    "&=& \\alpha(h_t) \\beta(h_t) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "$$\\lambda(h_t) = p(h_t|v_{1:T}) = \\frac{\\alpha(h_t) \\beta(h_t)}{p(v_{1:T})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(observations,trans_prob,emm_prob,init_prob,t,h_t):\n",
    "    states = init_prob.keys()\n",
    "    alphas = {}\n",
    "    betas = {}\n",
    "    for state in states: \n",
    "        alphas[state] = forward(observations,trans_prob,emm_prob,init_prob,t,state)\n",
    "        betas[state] = backward(observations,trans_prob,emm_prob,init_prob,t,state)\n",
    "    return alphas[h_t]*betas[h_t] / sum(alphas[state]*betas[state] for state in states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13557905771669326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothing(observations,trans_prob,emm_prob,init_prob,1,'m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise smoothing\n",
    "\\begin{eqnarray}\n",
    "p(h_t,h_{t+1},v_{1:T}) &=& p(v_{t+2:T},h_{t+1},v_{t+1},h_t,v_{1:t}) \\\\\n",
    "&=&p(v_{t+2:T}|v_{t+1},h_{t+1},h_t,v_{1:t})p(v_{t+1}|h_{t+1},h_{t},v_{1:t})p(h_{t+1}|h_t,v_{1:t})p(h_t,v_{1:t}) \\\\\n",
    "&=&p(v_{t+2:T}|h_{t+1})p(v_{t+1}|h_{t+1})p(h_{t+1}|h_t)p(h_t,v_{1:t}) \\\\\n",
    "&=& \\alpha(h_t) p(h_{t+1}|h_t) p(v_{t+1}|h_{t+1}) \\beta(h_{t+1})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_smooth(observations,trans_prob,emm_prob,init_prob,t,h_t,h_t_1):\n",
    "    alpha = forward(observations,trans_prob,emm_prob,init_prob,t,h_t)\n",
    "    beta = backward(observations,trans_prob,emm_prob,init_prob,t+1,h_t_1)\n",
    "    likely = likelihood(observations,trans_prob,emm_prob,init_prob,len(observations))\n",
    "    v_t_1 = observations[t] # note index\n",
    "    return (alpha * trans_prob[h_t][h_t_1] * emm_prob[h_t_1][v_t_1] * beta) / likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for cur_state in states:\n",
    "    for next_state in states:\n",
    "        s += pairwise_smooth(observations,trans_prob,emm_prob,init_prob,2,cur_state,next_state)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Likely Joint State ----- Second Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-Force \n",
    "Time complexity: $O(n^T)$,\n",
    "where $n$ is number of possible states and $T$ is the length of sequence. It is not tractable for long sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_maxProd(obs,tran_prob,emm_prob,states,init_prob):\n",
    "    max_prod = {}\n",
    "    for state_0 in states:\n",
    "        p_0 = init_prob[state_0] * emm_prob[state_0][obs[0]]\n",
    "        for state_1 in states:\n",
    "            p_1 = tran_prob[state_0][state_1] * emm_prob[state_1][obs[1]]\n",
    "            for state_2 in states:\n",
    "                p_2 = tran_prob[state_1][state_2] * emm_prob[state_2][obs[2]] \n",
    "                for state_3 in states:\n",
    "                    p_3 = tran_prob[state_2][state_3] * emm_prob[state_3][obs[3]]\n",
    "                    for state_4 in states:\n",
    "                        p_4 = tran_prob[state_3][state_4] * emm_prob[state_4][obs[4]]\n",
    "                        prob = p_0 * p_1 * p_2 * p_3 * p_4\n",
    "                        path = (state_0,state_1,state_2,state_3,state_4)\n",
    "                        max_prod[path] = prob \n",
    "\n",
    "    max_path = max(max_prod,key = max_prod.get)\n",
    "    return max_prod[max_path], max_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute max prod (0.0007257600000000002, ('h', 'l', 'n', 'l', 'n'))\n",
      "brute max prod time 0.001928091049194336\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"brute max prod\",brute_maxProd(observations,trans_prob,emm_prob,states,init_prob))\n",
    "end = time.time()\n",
    "print(\"brute max prod time\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Product Algorithm\n",
    "\\begin{eqnarray}\n",
    "\\max_{h_{1:T}}P(I,0|\\lambda) &=& \\max_{h_{1:T}}\\pi(i)\\prod_{t=1}^TP(h_{t+1}|h_{t})P(o_t|h_t) \\\\\n",
    " &=& \\max_{h_1}\\pi(h_1) ... \\max_{h_{T-1}}P(h_{T_1}|h_{T_2})P(o_{T-1}|h_{T-1})\\underline{\\max_{h_T}P(h_T|h_{T-1})P(o_T|h_T)} \\\\\n",
    " &=& \\max_{h_1}\\pi(h_1) ... \\underline{\\max_{h_{T-1}}P(h_{T_1}|h_{T_2})P(o_{T-1}|h_{T-1})\\mu(h_{T-1})} \\\\\n",
    " &=& \\max_{h_1}\\pi(h_1) ... \\mu(h_{T-2}) \\\\\n",
    "\\end{eqnarray}\n",
    "Time complexity: $O(T*n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_max_prod(observations, tran_prob, emm_prob, states, init_prob):\n",
    "    paths = defaultdict(list)\n",
    "    for i,obs in enumerate(reversed(observations[1:])):\n",
    "        cur_max = {}\n",
    "        for pre_state in states:\n",
    "            if i == 0:\n",
    "                cur_max[pre_state], max_state = max((tran_prob[pre_state][next_state]*\n",
    "                                                     emm_prob[next_state][obs], next_state)\n",
    "                                                        for next_state in states)\n",
    "                paths[pre_state].append(max_state)\n",
    "\n",
    "            else:\n",
    "                cur_max[pre_state], max_state = max((tran_prob[pre_state][next_state]*\n",
    "                                                    emm_prob[next_state][obs]*\n",
    "                                                    pre_max[next_state],next_state)\n",
    "                                                        for next_state in states)\n",
    "                paths[pre_state] = back_path[max_state] + [max_state]\n",
    "        pre_max = cur_max \n",
    "        back_path = dict(paths)\n",
    "    \n",
    "    # compute initial state\n",
    "    maxProd, first_state = max((init_prob[s]*emm_prob[s][observations[0]]*pre_max[s],s) for s in states)\n",
    "    paths[first_state].append(first_state)\n",
    "\n",
    "    return maxProd, list(reversed(paths[first_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward max prod (0.0007257600000000002, ['h', 'l', 'n', 'l', 'n'])\n",
      "time cost 0.0006361007690429688\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "start = time.time()\n",
    "print(\"backward max prod\",backward_max_prod(observations,trans_prob,emm_prob,states,init_prob))\n",
    "end = time.time()\n",
    "print(\"time cost\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programing (DP)\n",
    "def forward_max_prod(observations,tran_prob,emm_prob,states,init_prob):\n",
    "    paths = defaultdict(list)\n",
    "    for i,obs in enumerate(observations[:-1]):\n",
    "        cur_max = {}\n",
    "\n",
    "        for next_state in states:\n",
    "            if i == 0:\n",
    "                cur_max[next_state], max_state = max((init_prob[cur_state] *\n",
    "                                                      emm_prob[cur_state][obs] * \n",
    "                                                      tran_prob[cur_state][next_state], cur_state)\n",
    "                                                        for cur_state in states)\n",
    "                paths[next_state].append(max_state)\n",
    "\n",
    "            else:\n",
    "                cur_max[next_state], max_state = max((tran_prob[cur_state][next_state] *\n",
    "                                                      emm_prob[cur_state][obs] *\n",
    "                                                      pre_max[cur_state], cur_state)\n",
    "                                                        for cur_state in states)\n",
    "\n",
    "                paths[next_state] = pre_path[max_state] + [max_state]\n",
    "\n",
    "        pre_max = cur_max\n",
    "        pre_path = dict(paths) # copy the path \n",
    "    \n",
    "    # compute last state\n",
    "    max_likeli, max_state = max((emm_prob[last_state][observations[-1]]*pre_max[last_state],last_state)\n",
    "                                    for last_state in states)\n",
    "    paths[max_state].append(max_state)\n",
    "    return max_likeli, paths[max_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward max prod  (0.0007257600000000001, ['h', 'l', 'n', 'l', 'n'])\n",
      "forward max prod time  0.0009150505065917969\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"forward max prod \", forward_max_prod(observations,trans_prob,emm_prob,states,init_prob))\n",
    "end = time.time()\n",
    "print(\"forward max prod time \",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Baum-Welch algorithm ----- Third Problem \n",
    "#### Expectation Maximization(EM)\n",
    "$\\lambda = (\\pi,A,B)$, the parameters we are trying to learn\n",
    "\n",
    "Objective:   \n",
    "$Q(\\lambda,\\lambda^{'}) = E_{I}[\\log P(O,I|\\lambda)|O,\\lambda^{'}] = \\sum_{I}\\log P(O,I|\\lambda) P(I|0,\\lambda^{'})$\n",
    "\n",
    "Taking gradients with respect to $\\pi$, $A$ and $B$ to maximize Q function iteratively, we can get:\n",
    "\n",
    "1, Initial Probability update:   \n",
    "$\\pi_i = \\frac{\\sum_{n=1}^{N}P(h_1^{n} = i|O,\\lambda)}{N}$    \n",
    "\n",
    "2, Transition probability update:    \n",
    "$a_{ij} = \\frac{\\sum_{n=1}^{N}\\sum_{t=1}^{T-1}P(h_t^{n}=i,h_{t+1}^{n}=j|O,\\lambda)}{\\sum_{n=1}^{N}\\sum_{t=1}^{T-1}P(h_t^{n}=i|O,\\lambda)}$\n",
    "\n",
    "3, Emit probability update:   \n",
    "$b_i(k) = \\frac{\\sum_{n=1}^{N}\\sum_{t=1}^{T}P(h_t^{n} = i|O,\\lambda)I(o_t^{n} = k)}{\\sum_{n=1}^{N}\\sum_{t=1}^TP(h_t^{n}=i|O,\\lambda)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "def em_learn(observations,trans_prob,emm_prob,init_prob,obs_set):\n",
    "    \n",
    "    n = len(observations) # number of sequence\n",
    "    states = init_prob.keys()\n",
    "    \n",
    "    likely = 0\n",
    "    for obs in observations:\n",
    "        seq_len = len(obs)\n",
    "        # joint likelihood of all sequences\n",
    "        # Note: using negative log likelihood to avoid underflow\n",
    "        likely += -math.log(likelihood(obs,trans_prob,\n",
    "                                       emm_prob,init_prob,\n",
    "                                       seq_len))\n",
    "    \n",
    "    # update initial probability\n",
    "    #print('init')\n",
    "    new_init_prob = copy.deepcopy(init_prob)\n",
    "    for state in states:\n",
    "        first_prob = sum(smoothing(obs,trans_prob,\n",
    "                                   emm_prob,init_prob,1,state)\n",
    "                                   for obs in observations)\n",
    "        new_init_prob[state] = first_prob / n\n",
    "        \n",
    "    # update transition\n",
    "    #print('trans')\n",
    "    new_trans_prob = copy.deepcopy(trans_prob)\n",
    "    for cur_state in states:\n",
    "        smo_sum = sum(smoothing(obs,trans_prob,emm_prob,init_prob,t,\n",
    "                                cur_state) for obs in observations\n",
    "                                           for t in range(1,len(obs)))\n",
    "        \n",
    "        for next_state in states:\n",
    "            pair_sum = sum(pairwise_smooth(obs,trans_prob,\n",
    "                                           emm_prob,init_prob,\n",
    "                                           t,cur_state,next_state) \n",
    "                                           for obs in observations\n",
    "                                           for t in range(1,len(obs)))\n",
    "\n",
    "            new_trans_prob[cur_state][next_state] = pair_sum / smo_sum\n",
    "    \n",
    "    # update emission\n",
    "    #print('emission')\n",
    "    new_emm_prob = copy.deepcopy(emm_prob)\n",
    "    for state in states:\n",
    "        smo_sum = sum(smoothing(obs,trans_prob,emm_prob,init_prob,\n",
    "                        t,state) for obs in observations\n",
    "                                 for t in range(1,len(obs)+1))\n",
    "        for o in obs_set:\n",
    "            obs_sum = sum(smoothing(obs,trans_prob,emm_prob,init_prob,\n",
    "                            t,state) if obs[t-1] == o else 0 \n",
    "                            for obs in observations\n",
    "                            for t in range(1,len(obs)+1))\n",
    "    \n",
    "            new_emm_prob[state][o] = obs_sum / smo_sum\n",
    "        \n",
    "    new_likely = 0\n",
    "    for obs in observations:\n",
    "        seq_len = len(obs)\n",
    "        new_likely += -math.log(likelihood(obs,new_trans_prob,\n",
    "                                           new_emm_prob,\n",
    "                                           new_init_prob,seq_len))\n",
    "        \n",
    "    return likely,new_likely,new_trans_prob,new_emm_prob, new_init_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [[1,2,2,2,1],[2,1,1,2,1],[1,1,1,1,2]]\n",
    "\n",
    "def hmm_train(threshold,observations,trans_prob,emm_prob,init_prob,obs_set):\n",
    "    likely = 0\n",
    "    for obs in observations:\n",
    "        likely += -math.log(likelihood(obs,\n",
    "                                       trans_prob,\n",
    "                                       emm_prob,\n",
    "                                       init_prob,\n",
    "                                       len(obs)))\n",
    "    new_likely = 0\n",
    "\n",
    "    n = 1\n",
    "    while(likely - new_likely >= threshold):\n",
    "        print('step: {}'.format(n))\n",
    "        likely,new_likely,trans_prob,emm_prob,init_prob = em_learn(\n",
    "                                            observations,trans_prob,\n",
    "                                            emm_prob, init_prob,obs_set)\n",
    "        \n",
    "        print(\"previous step log likelihood: {:.5f}\".format(likely))\n",
    "        print(\"current step log likelihood: {:.5f}\".format(new_likely))\n",
    "        n+=1\n",
    "        #print(\"init \", init_prob)\n",
    "        #print(\"transition \", trans_prob)\n",
    "        #print(\"emit \", emm_prob)\n",
    "    return trans_prob, emm_prob, init_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n",
      "previous step log likelihood: 10.45997\n",
      "current step log likelihood: 10.08853\n",
      "step: 2\n",
      "previous step log likelihood: 10.08853\n",
      "current step log likelihood: 10.07064\n",
      "step: 3\n",
      "previous step log likelihood: 10.07064\n",
      "current step log likelihood: 10.06017\n",
      "step: 4\n",
      "previous step log likelihood: 10.06017\n",
      "current step log likelihood: 10.05320\n",
      "step: 5\n",
      "previous step log likelihood: 10.05320\n",
      "current step log likelihood: 10.04787\n",
      "step: 6\n",
      "previous step log likelihood: 10.04787\n",
      "current step log likelihood: 10.04326\n",
      "step: 7\n",
      "previous step log likelihood: 10.04326\n",
      "current step log likelihood: 10.03884\n",
      "step: 8\n",
      "previous step log likelihood: 10.03884\n",
      "current step log likelihood: 10.03431\n",
      "step: 9\n",
      "previous step log likelihood: 10.03431\n",
      "current step log likelihood: 10.02950\n",
      "step: 10\n",
      "previous step log likelihood: 10.02950\n",
      "current step log likelihood: 10.02426\n",
      "step: 11\n",
      "previous step log likelihood: 10.02426\n",
      "current step log likelihood: 10.01847\n",
      "step: 12\n",
      "previous step log likelihood: 10.01847\n",
      "current step log likelihood: 10.01205\n",
      "step: 13\n",
      "previous step log likelihood: 10.01205\n",
      "current step log likelihood: 10.00490\n",
      "step: 14\n",
      "previous step log likelihood: 10.00490\n",
      "current step log likelihood: 9.99692\n",
      "step: 15\n",
      "previous step log likelihood: 9.99692\n",
      "current step log likelihood: 9.98802\n",
      "step: 16\n",
      "previous step log likelihood: 9.98802\n",
      "current step log likelihood: 9.97811\n",
      "step: 17\n",
      "previous step log likelihood: 9.97811\n",
      "current step log likelihood: 9.96711\n",
      "step: 18\n",
      "previous step log likelihood: 9.96711\n",
      "current step log likelihood: 9.95494\n",
      "step: 19\n",
      "previous step log likelihood: 9.95494\n",
      "current step log likelihood: 9.94153\n",
      "step: 20\n",
      "previous step log likelihood: 9.94153\n",
      "current step log likelihood: 9.92683\n",
      "step: 21\n",
      "previous step log likelihood: 9.92683\n",
      "current step log likelihood: 9.91083\n",
      "step: 22\n",
      "previous step log likelihood: 9.91083\n",
      "current step log likelihood: 9.89350\n",
      "step: 23\n",
      "previous step log likelihood: 9.89350\n",
      "current step log likelihood: 9.87488\n",
      "step: 24\n",
      "previous step log likelihood: 9.87488\n",
      "current step log likelihood: 9.85501\n",
      "step: 25\n",
      "previous step log likelihood: 9.85501\n",
      "current step log likelihood: 9.83393\n",
      "step: 26\n",
      "previous step log likelihood: 9.83393\n",
      "current step log likelihood: 9.81174\n",
      "step: 27\n",
      "previous step log likelihood: 9.81174\n",
      "current step log likelihood: 9.78848\n",
      "step: 28\n",
      "previous step log likelihood: 9.78848\n",
      "current step log likelihood: 9.76420\n",
      "step: 29\n",
      "previous step log likelihood: 9.76420\n",
      "current step log likelihood: 9.73887\n",
      "step: 30\n",
      "previous step log likelihood: 9.73887\n",
      "current step log likelihood: 9.71239\n",
      "step: 31\n",
      "previous step log likelihood: 9.71239\n",
      "current step log likelihood: 9.68452\n",
      "step: 32\n",
      "previous step log likelihood: 9.68452\n",
      "current step log likelihood: 9.65487\n",
      "step: 33\n",
      "previous step log likelihood: 9.65487\n",
      "current step log likelihood: 9.62283\n",
      "step: 34\n",
      "previous step log likelihood: 9.62283\n",
      "current step log likelihood: 9.58762\n",
      "step: 35\n",
      "previous step log likelihood: 9.58762\n",
      "current step log likelihood: 9.54828\n",
      "step: 36\n",
      "previous step log likelihood: 9.54828\n",
      "current step log likelihood: 9.50382\n",
      "step: 37\n",
      "previous step log likelihood: 9.50382\n",
      "current step log likelihood: 9.45356\n",
      "step: 38\n",
      "previous step log likelihood: 9.45356\n",
      "current step log likelihood: 9.39754\n",
      "step: 39\n",
      "previous step log likelihood: 9.39754\n",
      "current step log likelihood: 9.33695\n",
      "step: 40\n",
      "previous step log likelihood: 9.33695\n",
      "current step log likelihood: 9.27419\n",
      "step: 41\n",
      "previous step log likelihood: 9.27419\n",
      "current step log likelihood: 9.21221\n",
      "step: 42\n",
      "previous step log likelihood: 9.21221\n",
      "current step log likelihood: 9.15336\n",
      "step: 43\n",
      "previous step log likelihood: 9.15336\n",
      "current step log likelihood: 9.09845\n",
      "step: 44\n",
      "previous step log likelihood: 9.09845\n",
      "current step log likelihood: 9.04676\n",
      "step: 45\n",
      "previous step log likelihood: 9.04676\n",
      "current step log likelihood: 8.99668\n",
      "step: 46\n",
      "previous step log likelihood: 8.99668\n",
      "current step log likelihood: 8.94655\n",
      "step: 47\n",
      "previous step log likelihood: 8.94655\n",
      "current step log likelihood: 8.89533\n",
      "step: 48\n",
      "previous step log likelihood: 8.89533\n",
      "current step log likelihood: 8.84312\n",
      "step: 49\n",
      "previous step log likelihood: 8.84312\n",
      "current step log likelihood: 8.79164\n",
      "step: 50\n",
      "previous step log likelihood: 8.79164\n",
      "current step log likelihood: 8.74385\n",
      "step: 51\n",
      "previous step log likelihood: 8.74385\n",
      "current step log likelihood: 8.70214\n",
      "step: 52\n",
      "previous step log likelihood: 8.70214\n",
      "current step log likelihood: 8.66645\n",
      "step: 53\n",
      "previous step log likelihood: 8.66645\n",
      "current step log likelihood: 8.63445\n",
      "step: 54\n",
      "previous step log likelihood: 8.63445\n",
      "current step log likelihood: 8.60293\n",
      "step: 55\n",
      "previous step log likelihood: 8.60293\n",
      "current step log likelihood: 8.56862\n",
      "step: 56\n",
      "previous step log likelihood: 8.56862\n",
      "current step log likelihood: 8.52816\n",
      "step: 57\n",
      "previous step log likelihood: 8.52816\n",
      "current step log likelihood: 8.47826\n",
      "step: 58\n",
      "previous step log likelihood: 8.47826\n",
      "current step log likelihood: 8.41687\n",
      "step: 59\n",
      "previous step log likelihood: 8.41687\n",
      "current step log likelihood: 8.34574\n",
      "step: 60\n",
      "previous step log likelihood: 8.34574\n",
      "current step log likelihood: 8.27274\n",
      "step: 61\n",
      "previous step log likelihood: 8.27274\n",
      "current step log likelihood: 8.20962\n",
      "step: 62\n",
      "previous step log likelihood: 8.20962\n",
      "current step log likelihood: 8.16428\n",
      "step: 63\n",
      "previous step log likelihood: 8.16428\n",
      "current step log likelihood: 8.13590\n",
      "step: 64\n",
      "previous step log likelihood: 8.13590\n",
      "current step log likelihood: 8.11866\n",
      "step: 65\n",
      "previous step log likelihood: 8.11866\n",
      "current step log likelihood: 8.10743\n",
      "step: 66\n",
      "previous step log likelihood: 8.10743\n",
      "current step log likelihood: 8.09944\n",
      "step: 67\n",
      "previous step log likelihood: 8.09944\n",
      "current step log likelihood: 8.09341\n",
      "step: 68\n",
      "previous step log likelihood: 8.09341\n",
      "current step log likelihood: 8.08871\n",
      "step: 69\n",
      "previous step log likelihood: 8.08871\n",
      "current step log likelihood: 8.08500\n",
      "step: 70\n",
      "previous step log likelihood: 8.08500\n",
      "current step log likelihood: 8.08203\n",
      "step: 71\n",
      "previous step log likelihood: 8.08203\n",
      "current step log likelihood: 8.07964\n",
      "step: 72\n",
      "previous step log likelihood: 8.07964\n",
      "current step log likelihood: 8.07770\n",
      "step: 73\n",
      "previous step log likelihood: 8.07770\n",
      "current step log likelihood: 8.07611\n",
      "step: 74\n",
      "previous step log likelihood: 8.07611\n",
      "current step log likelihood: 8.07482\n",
      "step: 75\n",
      "previous step log likelihood: 8.07482\n",
      "current step log likelihood: 8.07375\n",
      "step: 76\n",
      "previous step log likelihood: 8.07375\n",
      "current step log likelihood: 8.07287\n",
      "step: 77\n",
      "previous step log likelihood: 8.07287\n",
      "current step log likelihood: 8.07214\n",
      "step: 78\n",
      "previous step log likelihood: 8.07214\n",
      "current step log likelihood: 8.07154\n",
      "step: 79\n",
      "previous step log likelihood: 8.07154\n",
      "current step log likelihood: 8.07104\n",
      "step: 80\n",
      "previous step log likelihood: 8.07104\n",
      "current step log likelihood: 8.07063\n",
      "step: 81\n",
      "previous step log likelihood: 8.07063\n",
      "current step log likelihood: 8.07028\n",
      "step: 82\n",
      "previous step log likelihood: 8.07028\n",
      "current step log likelihood: 8.06999\n",
      "step: 83\n",
      "previous step log likelihood: 8.06999\n",
      "current step log likelihood: 8.06975\n",
      "step: 84\n",
      "previous step log likelihood: 8.06975\n",
      "current step log likelihood: 8.06955\n",
      "step: 85\n",
      "previous step log likelihood: 8.06955\n",
      "current step log likelihood: 8.06937\n",
      "step: 86\n",
      "previous step log likelihood: 8.06937\n",
      "current step log likelihood: 8.06923\n",
      "step: 87\n",
      "previous step log likelihood: 8.06923\n",
      "current step log likelihood: 8.06911\n",
      "step: 88\n",
      "previous step log likelihood: 8.06911\n",
      "current step log likelihood: 8.06901\n",
      "step: 89\n",
      "previous step log likelihood: 8.06901\n",
      "current step log likelihood: 8.06892\n",
      "step: 90\n",
      "previous step log likelihood: 8.06892\n",
      "current step log likelihood: 8.06885\n",
      "step: 91\n",
      "previous step log likelihood: 8.06885\n",
      "current step log likelihood: 8.06878\n",
      "step: 92\n",
      "previous step log likelihood: 8.06878\n",
      "current step log likelihood: 8.06873\n",
      "step: 93\n",
      "previous step log likelihood: 8.06873\n",
      "current step log likelihood: 8.06869\n",
      "step: 94\n",
      "previous step log likelihood: 8.06869\n",
      "current step log likelihood: 8.06865\n",
      "step: 95\n",
      "previous step log likelihood: 8.06865\n",
      "current step log likelihood: 8.06861\n",
      "step: 96\n",
      "previous step log likelihood: 8.06861\n",
      "current step log likelihood: 8.06858\n",
      "step: 97\n",
      "previous step log likelihood: 8.06858\n",
      "current step log likelihood: 8.06856\n",
      "step: 98\n",
      "previous step log likelihood: 8.06856\n",
      "current step log likelihood: 8.06854\n",
      "step: 99\n",
      "previous step log likelihood: 8.06854\n",
      "current step log likelihood: 8.06852\n",
      "step: 100\n",
      "previous step log likelihood: 8.06852\n",
      "current step log likelihood: 8.06850\n",
      "step: 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous step log likelihood: 8.06850\n",
      "current step log likelihood: 8.06848\n",
      "step: 102\n",
      "previous step log likelihood: 8.06848\n",
      "current step log likelihood: 8.06847\n",
      "step: 103\n",
      "previous step log likelihood: 8.06847\n",
      "current step log likelihood: 8.06846\n",
      "step: 104\n",
      "previous step log likelihood: 8.06846\n",
      "current step log likelihood: 8.06845\n",
      "step: 105\n",
      "previous step log likelihood: 8.06845\n",
      "current step log likelihood: 8.06844\n",
      "step: 106\n",
      "previous step log likelihood: 8.06844\n",
      "current step log likelihood: 8.06843\n",
      "step: 107\n",
      "previous step log likelihood: 8.06843\n",
      "current step log likelihood: 8.06843\n",
      "step: 108\n",
      "previous step log likelihood: 8.06843\n",
      "current step log likelihood: 8.06842\n",
      "step: 109\n",
      "previous step log likelihood: 8.06842\n",
      "current step log likelihood: 8.06842\n",
      "step: 110\n",
      "previous step log likelihood: 8.06842\n",
      "current step log likelihood: 8.06841\n",
      "step: 111\n",
      "previous step log likelihood: 8.06841\n",
      "current step log likelihood: 8.06841\n",
      "step: 112\n",
      "previous step log likelihood: 8.06841\n",
      "current step log likelihood: 8.06840\n",
      "step: 113\n",
      "previous step log likelihood: 8.06840\n",
      "current step log likelihood: 8.06840\n",
      "step: 114\n",
      "previous step log likelihood: 8.06840\n",
      "current step log likelihood: 8.06840\n",
      "step: 115\n",
      "previous step log likelihood: 8.06840\n",
      "current step log likelihood: 8.06839\n",
      "step: 116\n",
      "previous step log likelihood: 8.06839\n",
      "current step log likelihood: 8.06839\n",
      "step: 117\n",
      "previous step log likelihood: 8.06839\n",
      "current step log likelihood: 8.06839\n",
      "step: 118\n",
      "previous step log likelihood: 8.06839\n",
      "current step log likelihood: 8.06839\n",
      "step: 119\n",
      "previous step log likelihood: 8.06839\n",
      "current step log likelihood: 8.06838\n",
      "step: 120\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 121\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 122\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 123\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 124\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 125\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06838\n",
      "step: 126\n",
      "previous step log likelihood: 8.06838\n",
      "current step log likelihood: 8.06837\n"
     ]
    }
   ],
   "source": [
    "new_trans, new_emis, new_init = hmm_train(1e-6,observations,trans_prob,emm_prob,init_prob,obs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin init prob:\n",
      "{'n': 0.2, 'h': 0.4, 'm': 0.1, 'l': 0.3}\n",
      "learnt init prob:\n",
      "{'h': 0.6666666666666666, 'n': 0.0, 'm': 0.0, 'l': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print('origin init prob:')\n",
    "print(init_prob)\n",
    "print('learnt init prob:')\n",
    "print(new_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin trans prob:\n",
      "{'n': {'n': 0.2, 'h': 0.3, 'm': 0.1, 'l': 0.4}, 'h': {'n': 0.1, 'h': 0.5, 'm': 0.2, 'l': 0.2}, 'm': {'n': 0.2, 'h': 0.1, 'm': 0.5, 'l': 0.2}, 'l': {'n': 0.4, 'h': 0.2, 'm': 0.3, 'l': 0.1}}\n",
      "learnt trans prob:\n",
      "{'h': {'h': 0.5514183109981636, 'n': 0.055252808194144985, 'm': 0.3933288772937472, 'l': 3.5139441623745054e-09}, 'n': {'h': 7.885720427744287e-46, 'n': 6.259169588067811e-42, 'm': 5.478754060980203e-21, 'l': 1.0}, 'm': {'h': 3.046018480069598e-11, 'n': 0.9999999999695397, 'm': 4.077699007664078e-216, 'l': 3.529339693975087e-217}, 'l': {'h': 1.0, 'n': 6.486007001435362e-44, 'm': 5.116169013536259e-89, 'l': 1.1268166339588191e-85}}\n"
     ]
    }
   ],
   "source": [
    "print('origin trans prob:')\n",
    "print(trans_prob)\n",
    "print('learnt trans prob:')\n",
    "print(new_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin emit prob:\n",
      "{'n': {1: 0.5, 2: 0.5}, 'h': {1: 0.7, 2: 0.3}, 'm': {1: 0.6, 2: 0.4}, 'l': {1: 0.1, 2: 0.9}}\n",
      "learnt emit prob:\n",
      "{'h': {1: 1.0, 2: 1.4426237454521875e-28}, 'n': {1: 0.5119273653076248, 2: 0.48807263469237505}, 'm': {1: 2.4422515783041376e-07, 2: 0.9999997557748421}, 'l': {1: 3.653277514430187e-25, 2: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "print('origin emit prob:')\n",
    "print(emm_prob)\n",
    "print('learnt emit prob:')\n",
    "print(new_emis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech tagging:\n",
    "### Supervised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "# this file put in data directory\n",
    "with open('penn-data.json') as data_file:    \n",
    "    penn_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tagged data: 3914\n"
     ]
    }
   ],
   "source": [
    "print('total tagged data: {}'.format(len(penn_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "tags = []\n",
    "seqs = []\n",
    "poss = []\n",
    "test_poss = []\n",
    "test_seqs = []\n",
    "for i,p in enumerate(penn_data):\n",
    "    \n",
    "    sent = p[0].replace(',','').split()\n",
    "    sent[-1] = sent[-1].replace('.','')\n",
    "    assert len(sent) == len(p[1])\n",
    "    if i < 3800:\n",
    "        seqs.append(sent)\n",
    "        poss.append(p[1])\n",
    "        words += sent\n",
    "        tags += p[1]\n",
    "    else:\n",
    "        test_poss.append(p[1])\n",
    "        test_seqs.append(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(words))\n",
    "tag_set = list(set(tags))\n",
    "from collections import defaultdict\n",
    "trans_stat = {}\n",
    "emit_stat = {}\n",
    "init_stat = defaultdict(int)\n",
    "for tag in tag_set:\n",
    "    trans_stat[tag] = defaultdict(int)\n",
    "    emit_stat[tag] = defaultdict(int)\n",
    "# record emissions \n",
    "for word, tag in zip(words,tags):\n",
    "    emit_stat[tag][word] += 1\n",
    "# smooth unseen word\n",
    "for tag in tag_set:\n",
    "    emit_stat[tag]['<unk>'] = 1\n",
    "# record transitions\n",
    "for pos in poss:\n",
    "    # record first state\n",
    "    init_stat[pos[0]] += 1\n",
    "    for i in range(len(pos)-1):\n",
    "        trans_stat[pos[i]][pos[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 13160\n",
      "state size : 41\n"
     ]
    }
   ],
   "source": [
    "print('vocab size : {}'.format(len(vocab)))\n",
    "print('state size : {}'.format(len(tag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing \n",
    "for key in trans_stat.keys():\n",
    "    tag_tol = sum(trans_stat[key].values())\n",
    "    word_tol = sum(emit_stat[key].values())\n",
    "    for sub_key in trans_stat[key].keys():\n",
    "        trans_stat[key][sub_key] /= tag_tol\n",
    "    for sub_key in emit_stat[key].keys():\n",
    "        emit_stat[key][sub_key] /= word_tol\n",
    "        \n",
    "init_tol = sum(init_stat.values())\n",
    "for key in init_stat.keys():\n",
    "    init_stat[key] /= init_tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unseen(sents,vocab):\n",
    "    replaced = []\n",
    "    for sent in sents:\n",
    "        replaced.append([word if word in vocab else '<unk>' for word in sent])\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference \n",
    "test_seqs = replace_unseen(test_seqs,vocab)\n",
    "pred_tags = []\n",
    "for test in test_seqs:\n",
    "    pred_tags.append(backward_max_prod(test,trans_stat,emit_stat,tag_set,init_stat)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, labels):\n",
    "    corr = 0\n",
    "    tol = sum(len(pred) for pred in preds)\n",
    "    for pred, label in zip(preds, labels):\n",
    "        assert len(pred) == len(label)\n",
    "        corr += sum(p == l for p,l in zip(pred,label))\n",
    "    return corr / tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8768057784911717\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : {}'.format(evaluate(pred_tags,test_poss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: 0\n",
      "pos: ['RBS', 'JJ', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'JJ', 'NN']\n",
      "obs: ['<unk>', 'actual', 'profit', 'is', 'compared', 'with', 'the', '<unk>', 'estimate']\n",
      "seq: 1\n",
      "pos: ['NNP', 'NNP', 'NNP', 'VBD', 'PRP', 'VBD', 'PRP$', 'JJS', 'CD', 'WP$', 'NN', 'IN', 'RB', 'VBN', 'IN', 'NNP', 'NNP', 'DT', 'NNP', 'NN', 'VBG', 'NN']\n",
      "obs: ['First', 'Chicago', 'Corp.', 'said', 'it', 'completed', 'its', '<unk>', 'million', '<unk>', 'acquisition', 'of', 'closely', 'held', '<unk>', 'Financial', 'Corp.', 'another', 'Chicago', 'bank', 'holding', 'company']\n",
      "seq: 2\n",
      "pos: ['DT', 'NN', 'FW', '-RRB-', 'IN', 'DT', 'NNP', 'NNP', 'VBZ', 'VBG', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NN', \"''\"]\n",
      "obs: ['The', 'record', '<unk>', '<unk>', 'by', 'the', 'Soviet', 'Union', 'is', 'causing', 'serious', '<unk>', 'in', 'the', 'U.S.', 'grain', '<unk>']\n",
      "seq: 3\n",
      "pos: ['DT', 'JJ', 'NNS', 'VBP', 'RB', 'JJ', 'IN', 'PRP', 'VBP', 'VBG', 'TO', 'VB', 'RB', 'JJR', 'NNS', 'CC', 'VBG', 'TO', 'VB', 'DT', 'RB', 'JJR', 'NN', 'NN', 'TO', 'VB', 'IN', 'JJR', 'IN', 'JJ', 'NNS']\n",
      "obs: ['The', 'Soviet', 'purchases', 'are', 'so', 'massive', 'that', '<unk>', 'are', 'struggling', 'to', 'find', 'enough', '<unk>', '<unk>', 'and', '<unk>', 'to', 'move', 'the', 'recently', '<unk>', 'Midwest', 'crop', 'to', '<unk>', 'for', '<unk>', 'onto', 'Soviet', 'ships']\n",
      "seq: 4\n",
      "pos: ['NNP', 'WP$', 'NNS', 'VBP', 'VBN', 'CD', 'DT', 'NN', 'IN', 'DT', 'NN', 'JJR']\n",
      "obs: ['River', '<unk>', 'rates', 'have', 'soared', '40%', 'this', 'fall', 'from', 'a', 'year', 'earlier']\n",
      "seq: 5\n",
      "pos: ['WRB', 'NNS', 'CC', 'DT', 'NNS', 'VBP', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'NN']\n",
      "obs: ['<unk>', 'companies', 'and', 'some', '<unk>', 'are', '<unk>', 'a', 'sudden', '<unk>', 'of', 'business']\n",
      "seq: 6\n",
      "pos: ['CC', 'DT', 'NN', 'NNS', 'VBP', 'VBG', 'DT', 'NN', 'NNS', 'MD', 'VB', 'DT', 'NN', 'IN', 'FW', '-RRB-', 'TO', 'VB', 'RB', 'IN', 'DT', 'NN', 'TO', 'VB', 'PRP$', 'NNS', 'TO', 'DT', 'NNPS']\n",
      "obs: ['And', 'some', 'grain', 'analysts', 'are', 'predicting', 'that', 'corn', 'prices', 'might', '<unk>', 'this', 'month', 'as', '<unk>', '<unk>', 'to', 'find', 'enough', 'of', 'the', 'crop', 'to', 'meet', 'their', 'obligations', 'to', 'the', 'Soviets']\n",
      "seq: 7\n",
      "pos: ['DT', 'NNP', 'NNP', 'VBD', 'RB', 'CD', 'CD', 'NNS', 'IN', 'NNP', 'NN', 'IN', 'NNP', 'WDT', 'VBZ', 'DT', 'RBS', 'RB', 'VBN', 'TO', 'DT', 'NNP', 'NNP', 'IN', 'CD', 'NN', 'IN', 'DT', 'NNP']\n",
      "obs: ['The', 'Soviet', 'Union', 'bought', 'roughly', '310', 'million', 'bushels', 'of', 'U.S.', 'corn', 'in', 'October', 'which', 'is', 'the', 'most', 'ever', 'sold', 'to', 'the', 'Soviet', 'Union', 'in', 'one', 'month', 'from', 'the', 'US']\n",
      "seq: 8\n",
      "pos: ['DT', 'NNP', 'NNP', 'VBZ', 'RB', 'IN', 'PRP', 'VBN', 'IN', 'NNP', 'WDT', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJS', 'NNS']\n",
      "obs: ['The', 'Soviet', 'Union', 'wants', 'much', 'of', 'it', 'delivered', 'by', 'January', 'which', 'would', 'be', 'a', '<unk>', 'in', 'most', 'years']\n",
      "seq: 9\n",
      "pos: ['CC', 'PRP', 'VBZ', 'RB', 'JJ', 'DT', 'NN', 'IN', 'IN', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'IN', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NNP', 'NN', 'WDT', 'VBZ', 'VBN', 'JJ']\n",
      "obs: ['But', 'it', 'is', 'particularly', 'difficult', 'this', 'autumn', 'because', 'of', 'low', 'water', 'levels', 'on', 'the', '<unk>', 'River', 'on', 'which', 'flows', 'much', 'of', 'the', 'U.S.', 'corn', 'that', 'is', 'shipped', 'overseas']\n"
     ]
    }
   ],
   "source": [
    "for i,(obs,pos) in enumerate(zip(test_seqs[:10],pred_tags[:10])):\n",
    "    print('seq: {}'.format(i))\n",
    "    print('pos: {}'.format(pos))\n",
    "    print('obs: {}'.format(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('<unk>')\n",
    "#new_trans, new_emit, new_init = hmm_train(1e-6,seqs[:5],trans_stat,emit_stat,init_stat,vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
