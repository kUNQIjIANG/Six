{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient for linear regression\n",
    "#### General Objective Quadratic Form: \n",
    "\n",
    "$f = \\frac{1}{2} \\sum_{n}{(y_n - w^{T}x_n)^2} + \\frac{1}{2}\\lambda w^{t}w = \\frac{1}{2}w^{T}Aw - w^{T}b$\n",
    "\n",
    "$ A = \\lambda I + X^T X  $ \n",
    "\n",
    "Unique minimum exists if and only if A is postive definite.\n",
    "\n",
    "$ b = X^T Y$\n",
    "\n",
    "#### Line Search\n",
    "$ x_{k+1} = x_k + \\alpha_{k}p_k$ \n",
    "\n",
    "where $p_k$ is current searching direction and solving $a_k$ for step\n",
    "\n",
    "$ f_{(x_{k+1})} = \\frac{1}{2}(x_k + a_k p_k)^T A (x_k + a_k p_k) - (x_k + a_k p_k)^T b$\n",
    "\n",
    "$ \\frac{\\partial{f_{(x_{k+1})}}}{\\partial{a_k}} = p_{k}^T A x_k + p_{k}^T A a_k p_k - p_{k}^T b = 0 $\n",
    "\n",
    "$a_k = \\frac{p_{k}^T b - p_{k}^T A x_k}{p_{k}^T A p_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A,b):\n",
    "    dim = A.shape[0]\n",
    "    # init w with zero vector\n",
    "    w = np.zeros([dim,1])\n",
    "    # init p with negative gradient at initial w\n",
    "    g = A @ w - b\n",
    "    p = -g\n",
    "    old_val = 1e32\n",
    "    for i in range(100):\n",
    "        alpha = (p.T @ -g) / (p.T @ A @ p)\n",
    "        w = w + alpha * p\n",
    "        new_g = A @ w - b\n",
    "        beta = (new_g.T @ new_g) / (g.T @ g)\n",
    "        p = -new_g + beta * p\n",
    "        g = new_g\n",
    "        val = 0.5 * w.T @ (g-b)\n",
    "        if old_val - val < 1:\n",
    "            break\n",
    "        old_val = val\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse linear regression\n",
    "#### Conjugate gradient without $A$ to save N^2 memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_conjugate_gradient(datas,b, lamda):\n",
    "    dim = A.shape[0]\n",
    "    # init w with zero vector\n",
    "    w = np.zeros([dim,1])\n",
    "    # init p with negative gradient at initial w\n",
    "    g = A @ w - b\n",
    "    p = -g\n",
    "    old_val = 1e32\n",
    "    for i in range(100):\n",
    "        pap = lamda * p.T @ p\n",
    "        for n in range(len(datas)):\n",
    "            pap += (p.T @ datas[n])**2\n",
    "        # alpha = (p.T @ -g) / (p.T @ A @ p)\n",
    "        alpha = (p.T @ -g) / pap\n",
    "        w = w + alpha * p\n",
    "        # new_g = A @ w - b\n",
    "        new_g = -b + lamda * w\n",
    "        for n in range(len(datas)):\n",
    "            new_g += datas[n] * (datas[n].T @ w)\n",
    "        beta = (new_g.T @ new_g) / (g.T @ g)\n",
    "        p = -new_g + beta * p\n",
    "        g = new_g\n",
    "        val = 0.5 * w.T @ (g-b)\n",
    "        if old_val - val < 1:\n",
    "            break\n",
    "        old_val = val\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
