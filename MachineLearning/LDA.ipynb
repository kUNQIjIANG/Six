{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "This is a demostration of LDA topic model using Gibbs sampling on a \"perfect dataset\"   \n",
    "Thanks to the clear [tutorial](https://www.cnblogs.com/pinard/p/6831308.html) provided by Pinard Liu  \n",
    "Author: kUNQI jIANG   \n",
    "Date: 2019/1/22  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus generation\n",
    "As Gibbs sampling in LDA essentially based on bag-of-words so the order of words does not matter, I use completely seperated wordset of different topic to generate pure topic documents as corpus. This is the extreme case where words and topics will be completely clustered after LDA as we can see in the result. While in real word, a word exist in different topics, and a document can cover multi-topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_set = [\"broccoli\",\"banana\",\"spinach\",\"smoothie\",\"breakfast\",\"ham\",\"cream\",\"eat\",\"vegetable\",\"dinner\",\"lunch\",\n",
    "            \"apple\",\"peach\",\"pork\",\"beef\",\"rice\",\"noodle\",\"chicken\",\"KFC\",\"restaurant\",\"cream\",\"tea\",\"pan\",\"beacon\"]\n",
    "animal_set = [\"dog\",\"cat\",\"fish\",\"chinchilla\",\"kitten\",\"cute\",\"hamster\",\"munching\",\"bird\",\"elephant\",\"monkey\",\"zoo\",\n",
    "              \"zoology\",\"pig\",\"piggy\",\"duck\",\"mice\",\"micky\",\"tiger\",\"lion\",\"horse\",\"dragon\",\"panda\",\"bee\",\"rabbit\"]\n",
    "soccer_set = [\"football\",\"pitch\",\"play\",\"player\",\"cup\",\"ballon\",\"messi\",\"ronald\",\"manU\",\"liverpool\",\"chelase\",\"ozil\",\n",
    "              \"practice\",\"hard\",\"dream\",\"stadium\",\"fast\",\"speed\",\"strong\",\"move\",\"shot\",\"attack\",\"defense\",\"win\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate(topic_set):\n",
    "    sent = np.random.choice(topic_set,10)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_set = [food_set,animal_set,soccer_set]\n",
    "corpus = []\n",
    "for i in range(100):\n",
    "    corpus.append(generate(topics_set[0]).split())\n",
    "    corpus.append(generate(topics_set[1]).split())\n",
    "    corpus.append(generate(topics_set[2]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_words = [word for document in corpus for word in document]\n",
    "vocab = set(all_words)\n",
    "num_docs = len(corpus)\n",
    "num_words = len(vocab)\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 latent topics \n",
    "num_topics = 3\n",
    "# Dirichlet prior\n",
    "alpha = np.ones([num_topics])\n",
    "#ita = term_freq\n",
    "ita = 0.1 * np.ones([num_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random assignment\n",
    "At the start randomly assign topic to each word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = []\n",
    "docs_topics = np.zeros([num_docs,num_topics]) # counts of topic assignments of each word in each doc\n",
    "words_topics = np.zeros([num_words,num_topics]) # counts of topic distributes of each word over all doc\n",
    "topics_words = np.zeros([num_topics,num_words]) # counts of word distributes of each topic over all doc\n",
    "\n",
    "for d,document in enumerate(corpus):\n",
    "    theta = np.random.dirichlet(alpha, 1)[0]\n",
    "    doc_topics = []\n",
    "    for n,word in enumerate(document):\n",
    "        sample = np.random.multinomial(1, theta, size=1)[0]\n",
    "        topic = list(sample).index(1)\n",
    "        doc_topics.append(topic)\n",
    "        docs_topics[d,topic] += 1\n",
    "        words_topics[word2id[word],topic] += 1\n",
    "        topics_words[topic,word2id[word]] += 1\n",
    "    topic_assignments.append(doc_topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita):\n",
    "    topic_probs = (docs_topics[d] + alpha) / np.sum(docs_topics[d] + alpha)\n",
    "    word_sum = np.sum(topics_words + ita, axis = 1)\n",
    "    word_probs = (words_topics[word_id] + ita[word_id]) / word_sum\n",
    "    # posterior probs\n",
    "    probs = topic_probs * word_probs\n",
    "    # normalize\n",
    "    sample_probs = probs / np.sum(probs)\n",
    "    #print(sample_probs)\n",
    "    # sample new topic for current word\n",
    "    new_topic = list(np.random.multinomial(1, sample_probs, size=1)[0]).index(1)\n",
    "    return new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 15\n",
    "for j in range(num_iterations):\n",
    "    for d in range(len(corpus)):\n",
    "        document = corpus[d]\n",
    "        for n in range(len(document)):\n",
    "            word = document[n]\n",
    "            word_id = word2id[word]\n",
    "            topic = topic_assignments[d][n]\n",
    "            # exclude current word and topic\n",
    "            docs_topics[d][topic] -= 1\n",
    "            topics_words[topic][word_id] -=1\n",
    "            words_topics[word_id,topic] -= 1\n",
    "            new_topic = Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita)\n",
    "            # update topic and word state\n",
    "            docs_topics[d][new_topic] += 1\n",
    "            topics_words[new_topic][word_id] += 1\n",
    "            words_topics[word_id,new_topic] += 1\n",
    "            topic_assignments[d][n] = new_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [10.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all seperated\n",
    "docs_topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "['cream', 'chicken', 'peach', 'restaurant', 'breakfast', 'lunch', 'broccoli', 'smoothie', 'banana', 'KFC', 'noodle', 'eat', 'dinner', 'vegetable', 'beacon', 'spinach', 'pan', 'tea', 'pork', 'beef', 'apple', 'rice', 'ham', 'liverpool', 'bird', 'defense', 'ronald', 'rabbit', 'stadium', 'fish', 'ballon', 'fast', 'move', 'cat', 'speed', 'dog', 'micky', 'win', 'monkey', 'chinchilla', 'kitten', 'attack', 'piggy', 'horse', 'zoology', 'duck', 'football', 'mice', 'tiger', 'munching', 'player', 'play', 'zoo', 'ozil', 'messi', 'pitch', 'chelase', 'cup', 'hamster', 'cute', 'shot', 'dream', 'dragon', 'bee', 'lion', 'practice', 'hard', 'elephant', 'manU', 'pig', 'strong', 'panda']\n",
      "Topic:  1\n",
      "['hamster', 'fish', 'panda', 'micky', 'monkey', 'zoology', 'cute', 'rabbit', 'horse', 'lion', 'elephant', 'chinchilla', 'mice', 'tiger', 'dragon', 'kitten', 'bee', 'pig', 'cat', 'bird', 'piggy', 'zoo', 'dog', 'munching', 'duck', 'banana', 'liverpool', 'breakfast', 'peach', 'defense', 'vegetable', 'ronald', 'stadium', 'broccoli', 'apple', 'ballon', 'fast', 'move', 'speed', 'lunch', 'chicken', 'win', 'attack', 'pan', 'KFC', 'cream', 'beef', 'football', 'player', 'tea', 'play', 'ozil', 'messi', 'smoothie', 'eat', 'pitch', 'chelase', 'cup', 'dinner', 'spinach', 'shot', 'dream', 'rice', 'ham', 'beacon', 'pork', 'practice', 'noodle', 'hard', 'restaurant', 'manU', 'strong']\n",
      "Topic:  2\n",
      "['practice', 'chelase', 'dream', 'football', 'win', 'defense', 'speed', 'messi', 'pitch', 'shot', 'stadium', 'ozil', 'cup', 'player', 'liverpool', 'strong', 'fast', 'ronald', 'ballon', 'manU', 'hard', 'play', 'attack', 'move', 'pan', 'banana', 'breakfast', 'bird', 'peach', 'vegetable', 'rabbit', 'broccoli', 'apple', 'fish', 'cat', 'lunch', 'dog', 'micky', 'chicken', 'monkey', 'chinchilla', 'kitten', 'piggy', 'KFC', 'horse', 'zoology', 'cream', 'duck', 'beef', 'mice', 'tiger', 'munching', 'tea', 'zoo', 'smoothie', 'eat', 'hamster', 'dinner', 'spinach', 'cute', 'rice', 'ham', 'beacon', 'dragon', 'bee', 'lion', 'pork', 'noodle', 'restaurant', 'elephant', 'pig', 'panda']\n"
     ]
    }
   ],
   "source": [
    "for i,state in enumerate(topics_words):\n",
    "    # sorted descending word frequence within each topic\n",
    "    topic_id_freq = sorted(range(len(state)), key=lambda k: state[k], reverse=True)\n",
    "    topic_word_freq = [id2word[i] for i in topic_id_freq]\n",
    "    print(\"Topic: \", i)\n",
    "    print(topic_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46.,  0., 48.,  0., 55.,  0., 39.,  0.,  0.,  0., 47., 29.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0., 48.,  0.,  0., 59.,  0.,  0.,  0.,  0.,\n",
       "         0., 36.,  0., 43.,  0.,  0., 92.,  0., 32.,  0.,  0.,  0.,  0.,\n",
       "         0., 35.,  0.,  0.,  0.,  0., 47., 41.,  0.,  0.,  0.,  0., 40.,\n",
       "        37.,  0.,  0.,  0., 29., 29., 38.,  0.,  0.,  0., 33.,  0., 43.,\n",
       "         0., 53.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 34.,  0.,  0.,  0.,  0., 42.,  0.,  0.,  0., 50.,\n",
       "         0.,  0.,  0., 35.,  0.,  0., 33., 48.,  0.,  0., 46., 40., 36.,\n",
       "         0.,  0., 34.,  0., 42., 46.,  0., 30.,  0.,  0., 38., 38., 32.,\n",
       "         0.,  0.,  0., 34.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 53.,  0.,\n",
       "         0., 46.,  0.,  0.,  0.,  0.,  0., 37., 36., 42.,  0.,  0.,  0.,\n",
       "         0.,  0., 42.,  0., 36.,  0., 50.],\n",
       "       [ 0., 39.,  0.,  0.,  0., 45.,  0., 37.,  0., 43.,  0.,  0.,  0.,\n",
       "        36., 38., 26.,  0., 45.,  0.,  0.,  0.,  0., 46.,  0.,  0.,  0.,\n",
       "        32.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 47.,  0.,  0.,  0.,\n",
       "        40.,  0., 34.,  0., 41., 45.,  0.,  0., 45., 56., 41.,  0.,  0.,\n",
       "         0.,  0., 44., 53.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 57.,  0.,\n",
       "        35.,  0.,  0., 36.,  0., 39.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.  0.  0.] banana\n",
      "[ 0.  0. 39.] liverpool\n",
      "[48.  0.  0.] breakfast\n",
      "[ 0. 34.  0.] bird\n",
      "[55.  0.  0.] peach\n",
      "[ 0.  0. 45.] defense\n",
      "[39.  0.  0.] vegetable\n",
      "[ 0.  0. 37.] ronald\n",
      "[ 0. 42.  0.] rabbit\n",
      "[ 0.  0. 43.] stadium\n",
      "[47.  0.  0.] broccoli\n",
      "[29.  0.  0.] apple\n",
      "[ 0. 50.  0.] fish\n",
      "[ 0.  0. 36.] ballon\n",
      "[ 0.  0. 38.] fast\n",
      "[ 0.  0. 26.] move\n",
      "[ 0. 35.  0.] cat\n",
      "[ 0.  0. 45.] speed\n",
      "[48.  0.  0.] lunch\n",
      "[ 0. 33.  0.] dog\n",
      "[ 0. 48.  0.] micky\n",
      "[59.  0.  0.] chicken\n",
      "[ 0.  0. 46.] win\n",
      "[ 0. 46.  0.] monkey\n",
      "[ 0. 40.  0.] chinchilla\n",
      "[ 0. 36.  0.] kitten\n",
      "[ 0.  0. 32.] attack\n",
      "[36.  0.  1.] pan\n",
      "[ 0. 34.  0.] piggy\n",
      "[43.  0.  0.] KFC\n",
      "[ 0. 42.  0.] horse\n",
      "[ 0. 46.  0.] zoology\n",
      "[92.  0.  0.] cream\n",
      "[ 0. 30.  0.] duck\n",
      "[32.  0.  0.] beef\n",
      "[ 0.  0. 47.] football\n",
      "[ 0. 38.  0.] mice\n",
      "[ 0. 38.  0.] tiger\n",
      "[ 0. 32.  0.] munching\n",
      "[ 0.  0. 40.] player\n",
      "[35.  0.  0.] tea\n",
      "[ 0.  0. 34.] play\n",
      "[ 0. 34.  0.] zoo\n",
      "[ 0.  0. 41.] ozil\n",
      "[ 0.  0. 45.] messi\n",
      "[47.  0.  0.] smoothie\n",
      "[41.  0.  0.] eat\n",
      "[ 0.  0. 45.] pitch\n",
      "[ 0.  0. 56.] chelase\n",
      "[ 0.  0. 41.] cup\n",
      "[ 0. 53.  0.] hamster\n",
      "[40.  0.  0.] dinner\n",
      "[37.  0.  0.] spinach\n",
      "[ 0. 46.  0.] cute\n",
      "[ 0.  0. 44.] shot\n",
      "[ 0.  0. 53.] dream\n",
      "[29.  0.  0.] rice\n",
      "[29.  0.  0.] ham\n",
      "[38.  0.  0.] beacon\n",
      "[ 0. 37.  0.] dragon\n",
      "[ 0. 36.  0.] bee\n",
      "[ 0. 42.  0.] lion\n",
      "[33.  0.  0.] pork\n",
      "[ 0.  0. 57.] practice\n",
      "[43.  0.  0.] noodle\n",
      "[ 0.  0. 35.] hard\n",
      "[53.  0.  0.] restaurant\n",
      "[ 0. 42.  0.] elephant\n",
      "[ 0.  0. 36.] manU\n",
      "[ 0. 36.  0.] pig\n",
      "[ 0.  0. 39.] strong\n",
      "[ 0. 50.  0.] panda\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words_topics)):\n",
    "    print(words_topics[i],id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Justify my result with gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.052*\"hamster\" + 0.049*\"panda\" + 0.049*\"fish\" + 0.047*\"micky\" + 0.045*\"zoology\" + 0.045*\"cute\" + 0.045*\"monkey\" + 0.041*\"lion\" + 0.041*\"horse\" + 0.041*\"rabbit\" + 0.041*\"elephant\" + 0.039*\"chinchilla\" + 0.037*\"mice\" + 0.037*\"tiger\" + 0.036*\"dragon\" + 0.035*\"pig\" + 0.035*\"kitten\" + 0.035*\"bee\" + 0.034*\"cat\" + 0.033*\"bird\" + 0.033*\"zoo\" + 0.033*\"piggy\" + 0.032*\"dog\" + 0.032*\"munching\" + 0.030*\"duck\" + 0.001*\"practice\" + 0.000*\"manU\" + 0.000*\"cream\" + 0.000*\"ballon\" + 0.000*\"attack\" + 0.000*\"speed\" + 0.000*\"shot\" + 0.000*\"defense\" + 0.000*\"dream\" + 0.000*\"ozil\" + 0.000*\"liverpool\" + 0.000*\"football\" + 0.000*\"chelase\" + 0.000*\"play\" + 0.000*\"player\" + 0.000*\"dinner\" + 0.000*\"breakfast\" + 0.000*\"pitch\" + 0.000*\"chicken\" + 0.000*\"KFC\" + 0.000*\"cup\" + 0.000*\"hard\" + 0.000*\"peach\" + 0.000*\"beef\" + 0.000*\"eat\" + 0.000*\"apple\" + 0.000*\"tea\" + 0.000*\"move\" + 0.000*\"beacon\" + 0.000*\"stadium\" + 0.000*\"ronald\" + 0.000*\"pan\" + 0.000*\"win\" + 0.000*\"strong\" + 0.000*\"smoothie\" + 0.000*\"restaurant\" + 0.000*\"broccoli\" + 0.000*\"banana\" + 0.000*\"spinach\" + 0.000*\"rice\" + 0.000*\"fast\" + 0.000*\"lunch\" + 0.000*\"vegetable\" + 0.000*\"ham\" + 0.000*\"noodle\" + 0.000*\"pork\" + 0.000*\"messi\"')\n",
      "(1, '0.090*\"cream\" + 0.058*\"chicken\" + 0.054*\"peach\" + 0.052*\"restaurant\" + 0.047*\"breakfast\" + 0.047*\"lunch\" + 0.046*\"smoothie\" + 0.046*\"broccoli\" + 0.045*\"banana\" + 0.042*\"noodle\" + 0.042*\"KFC\" + 0.040*\"eat\" + 0.039*\"dinner\" + 0.038*\"vegetable\" + 0.037*\"beacon\" + 0.036*\"pan\" + 0.036*\"spinach\" + 0.034*\"tea\" + 0.032*\"pork\" + 0.031*\"beef\" + 0.029*\"ham\" + 0.029*\"rice\" + 0.028*\"apple\" + 0.001*\"cat\" + 0.001*\"elephant\" + 0.001*\"chinchilla\" + 0.000*\"bee\" + 0.000*\"dragon\" + 0.000*\"micky\" + 0.000*\"rabbit\" + 0.000*\"horse\" + 0.000*\"monkey\" + 0.000*\"hamster\" + 0.000*\"kitten\" + 0.000*\"zoo\" + 0.000*\"fish\" + 0.000*\"tiger\" + 0.000*\"dream\" + 0.000*\"dog\" + 0.000*\"cute\" + 0.000*\"duck\" + 0.000*\"practice\" + 0.000*\"piggy\" + 0.000*\"chelase\" + 0.000*\"messi\" + 0.000*\"player\" + 0.000*\"fast\" + 0.000*\"pitch\" + 0.000*\"hard\" + 0.000*\"pig\" + 0.000*\"mice\" + 0.000*\"cup\" + 0.000*\"speed\" + 0.000*\"munching\" + 0.000*\"zoology\" + 0.000*\"stadium\" + 0.000*\"lion\" + 0.000*\"panda\" + 0.000*\"bird\" + 0.000*\"strong\" + 0.000*\"manU\" + 0.000*\"attack\" + 0.000*\"ronald\" + 0.000*\"liverpool\" + 0.000*\"ozil\" + 0.000*\"shot\" + 0.000*\"ballon\" + 0.000*\"win\" + 0.000*\"football\" + 0.000*\"play\" + 0.000*\"defense\" + 0.000*\"move\"')\n",
      "(2, '0.055*\"practice\" + 0.054*\"chelase\" + 0.052*\"dream\" + 0.046*\"football\" + 0.045*\"win\" + 0.044*\"messi\" + 0.044*\"defense\" + 0.044*\"pitch\" + 0.044*\"speed\" + 0.043*\"shot\" + 0.042*\"stadium\" + 0.040*\"cup\" + 0.040*\"ozil\" + 0.039*\"player\" + 0.038*\"strong\" + 0.038*\"liverpool\" + 0.037*\"fast\" + 0.036*\"ronald\" + 0.035*\"ballon\" + 0.035*\"manU\" + 0.034*\"hard\" + 0.033*\"play\" + 0.031*\"attack\" + 0.025*\"move\" + 0.001*\"spinach\" + 0.001*\"pan\" + 0.001*\"peach\" + 0.001*\"beacon\" + 0.001*\"lunch\" + 0.001*\"eat\" + 0.001*\"pork\" + 0.001*\"vegetable\" + 0.001*\"banana\" + 0.001*\"broccoli\" + 0.001*\"KFC\" + 0.001*\"restaurant\" + 0.001*\"apple\" + 0.001*\"smoothie\" + 0.001*\"noodle\" + 0.001*\"dinner\" + 0.001*\"beef\" + 0.001*\"rice\" + 0.001*\"cream\" + 0.001*\"chicken\" + 0.000*\"tea\" + 0.000*\"ham\" + 0.000*\"breakfast\" + 0.000*\"hamster\" + 0.000*\"piggy\" + 0.000*\"rabbit\" + 0.000*\"dragon\" + 0.000*\"munching\" + 0.000*\"zoology\" + 0.000*\"fish\" + 0.000*\"pig\" + 0.000*\"lion\" + 0.000*\"bird\" + 0.000*\"tiger\" + 0.000*\"kitten\" + 0.000*\"dog\" + 0.000*\"elephant\" + 0.000*\"bee\" + 0.000*\"horse\" + 0.000*\"monkey\" + 0.000*\"mice\" + 0.000*\"panda\" + 0.000*\"chinchilla\" + 0.000*\"micky\" + 0.000*\"zoo\" + 0.000*\"cute\" + 0.000*\"cat\" + 0.000*\"duck\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "text_data = corpus\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "id_corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(id_corpus, num_topics = num_topics, id2word=dictionary, passes=12)\n",
    "#ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=num_words)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "As we have had trained topics-words distributes, we only need to use them \n",
    "to Gibbs sample the topic for each word in the test documents until converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [[\"ozil\",\"panda\",\"pig\",\"ballon\",\"attack\",\"eat\",\"dragon\",\"ronald\",\"micky\",\"dinner\",\"bird\",\"messi\"],\n",
    "               [\"vegetable\",\"liverpool\",\"mice\",\"chelase\",\"speed\",\"horse\",\"rice\",\"peach\",\"noodle\",\"beacon\",\"bee\"],\n",
    "               [\"defense\",\"win\",\"hard\",\"rabbit\",\"player\",\"strong\",\"lion\",\"zoo\",\"pig\",\"cat\",\"player\",\"manU\",\"shot\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly random assign topic to each word in test docs\n",
    "test_num_docs = len(test_corpus)\n",
    "test_topic_assignments = []\n",
    "test_docs_topics = np.zeros([test_num_docs,num_topics]) # counts of topic assignments of each word in each doc\n",
    "\n",
    "for d,document in enumerate(test_corpus):\n",
    "    theta = np.random.dirichlet(alpha, 1)[0]\n",
    "    test_doc_topics = []\n",
    "    for n,word in enumerate(document):\n",
    "        sample = np.random.multinomial(1, theta, size=1)[0]\n",
    "        topic = list(sample).index(1)\n",
    "        test_doc_topics.append(topic)\n",
    "        test_docs_topics[d,topic] += 1\n",
    "\n",
    "    test_topic_assignments.append(test_doc_topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       " [1, 2, 0, 0, 2, 2, 0, 0, 1, 2, 2, 0, 0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_topic_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "for j in range(num_iterations):\n",
    "    for d in range(len(test_corpus)):\n",
    "        document = test_corpus[d]\n",
    "        for n in range(len(document)):\n",
    "            word = document[n]\n",
    "            word_id = word2id[word]\n",
    "            topic = test_topic_assignments[d][n]\n",
    "            # exclude current word and topic\n",
    "            test_docs_topics[d][topic] -= 1\n",
    "            new_topic = Gibbs_sampling(d,word_id,words_topics,test_docs_topics,topics_words,alpha,ita)\n",
    "            # update topic and word state\n",
    "            test_docs_topics[d][new_topic] += 1\n",
    "            test_topic_assignments[d][n] = new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 1, 2, 2, 0, 1, 2, 1, 0, 1, 2],\n",
       " [0, 2, 1, 2, 2, 1, 0, 0, 0, 0, 1],\n",
       " [2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_topic_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_doc_0: topic_0:0.17, topic_1:0.42, topic_2:0.42\n",
      "test_doc_1: topic_0:0.45, topic_1:0.27, topic_2:0.27\n",
      "test_doc_2: topic_0:0.00, topic_1:0.38, topic_2:0.62\n"
     ]
    }
   ],
   "source": [
    "for i in range(test_num_docs):\n",
    "    s_doc = \"test_doc_\"+str(i)+\": \"\n",
    "    topic_dis = test_docs_topics[i] / np.sum(test_docs_topics[i])\n",
    "    s_dis = \"topic_0:{0:.2f}, topic_1:{1:.2f}, topic_2:{2:.2f}\".format(topic_dis[0],topic_dis[1],topic_dis[2])\n",
    "    print(s_doc + s_dis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
