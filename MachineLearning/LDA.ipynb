{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "This is a demostration of LDA topic model using Gibbs sampling on a \"perfect dataset\"   \n",
    "Thanks to the clear [tutorial](https://www.cnblogs.com/pinard/p/6831308.html) provided by Pinard Liu  \n",
    "Author: kUNQI jIANG   \n",
    "Date: 2019/1/22  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus generation\n",
    "As Gibbs sampling in LDA essentially based on bag-of-words so the order of words does not matter, I use completely seperated wordset of different topic to generate pure topic documents as corpus. This is the extreme case where words and topics will be completely clustered after LDA as we can see in the result. While in real word, a word exist in different topics, and a document can cover multi-topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_set = [\"broccoli\",\"banana\",\"spinach\",\"smoothie\",\"breakfast\",\"ham\",\"cream\",\"eat\",\"vegetable\",\"dinner\",\"lunch\",\n",
    "            \"apple\",\"peach\",\"pork\",\"beef\",\"rice\",\"noodle\",\"chicken\",\"KFC\",\"restaurant\",\"cream\",\"tea\",\"pan\",\"beacon\"]\n",
    "animal_set = [\"dog\",\"cat\",\"fish\",\"chinchilla\",\"kitten\",\"cute\",\"hamster\",\"munching\",\"bird\",\"elephant\",\"monkey\",\"zoo\",\n",
    "              \"zoology\",\"pig\",\"piggy\",\"duck\",\"mice\",\"micky\",\"tiger\",\"lion\",\"horse\",\"dragon\",\"panda\",\"bee\",\"rabbit\"]\n",
    "soccer_set = [\"football\",\"pitch\",\"play\",\"player\",\"cup\",\"ballon\",\"messi\",\"ronald\",\"manU\",\"liverpool\",\"chelase\",\"ozil\",\n",
    "              \"practice\",\"hard\",\"dream\",\"stadium\",\"fast\",\"speed\",\"strong\",\"move\",\"shot\",\"attack\",\"defense\",\"win\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate(topic_set):\n",
    "    sent = np.random.choice(topic_set,10)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_set = [food_set,animal_set,soccer_set]\n",
    "corpus = []\n",
    "for i in range(100):\n",
    "    corpus.append(generate(topics_set[0]).split())\n",
    "    corpus.append(generate(topics_set[1]).split())\n",
    "    corpus.append(generate(topics_set[2]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_words = [word for document in corpus for word in document]\n",
    "vocab = set(all_words)\n",
    "num_docs = len(corpus)\n",
    "num_words = len(vocab)\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 latent topics \n",
    "num_topics = 3\n",
    "# Dirichlet prior\n",
    "alpha = np.ones([num_topics])\n",
    "ita = 0.1 * np.ones([num_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random assignment\n",
    "At the start randomly assign topic to each word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = []\n",
    "docs_topics = np.zeros([num_docs,num_topics]) # counts of topic assignments of each word in each doc\n",
    "words_topics = np.zeros([num_words,num_topics]) # counts of topic distributes of each word over all doc\n",
    "topics_words = np.zeros([num_topics,num_words]) # counts of word distributes of each topic over all doc\n",
    "\n",
    "for d,document in enumerate(corpus):\n",
    "    theta = np.random.dirichlet(alpha, 1)[0]\n",
    "    doc_topics = []\n",
    "    for n,word in enumerate(document):\n",
    "        sample = np.random.multinomial(1, theta, size=1)[0]\n",
    "        topic = list(sample).index(1)\n",
    "        doc_topics.append(topic)\n",
    "        docs_topics[d,topic] += 1\n",
    "        words_topics[word2id[word],topic] += 1\n",
    "        topics_words[topic,word2id[word]] += 1\n",
    "    topic_assignments.append(doc_topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita):\n",
    "    \n",
    "    topic_probs = (docs_topics[d] + alpha) / np.sum(docs_topics[d] + alpha)\n",
    "    word_sum = np.sum(topics_words + ita, axis = 1)\n",
    "    word_probs = (words_topics[word_id] + ita[word_id]) / word_sum\n",
    "    # posterior probs\n",
    "    probs = topic_probs * word_probs\n",
    "    # normalize\n",
    "    sample_probs = probs / np.sum(probs)\n",
    "    #print(sample_probs)\n",
    "    # sample new topic for current word\n",
    "    new_topic = list(np.random.multinomial(1, sample_probs, size=1)[0]).index(1)\n",
    "    return new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "stop = 1\n",
    "i = 0\n",
    "num_iterations = 9\n",
    "for j in range(num_iterations):\n",
    "    for d in range(len(corpus)):\n",
    "        document = corpus[d]\n",
    "        for n in range(len(document)):\n",
    "            word = document[n]\n",
    "            word_id = word2id[word]\n",
    "            topic = topic_assignments[d][n]\n",
    "            # exclude current word and topic\n",
    "            docs_topics[d][topic] -= 1\n",
    "            topics_words[topic][word_id] -=1\n",
    "            words_topics[word_id,topic] -= 1\n",
    "            new_topic = Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita)\n",
    "            # update topic and word state\n",
    "            docs_topics[d][new_topic] += 1\n",
    "            topics_words[new_topic][word_id] += 1\n",
    "            words_topics[word_id,new_topic] += 1\n",
    "            topic_assignments[d][n] = new_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [ 9.,  0.,  1.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 1.,  0.,  9.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "['speed', 'strong', 'chelase', 'stadium', 'dream', 'ronald', 'player', 'messi', 'attack', 'hard', 'fast', 'liverpool', 'ozil', 'manU', 'win', 'pitch', 'play', 'defense', 'cup', 'shot', 'practice', 'ballon', 'move', 'football', 'lunch', 'vegetable', 'lion', 'fish', 'peach', 'banana', 'micky', 'munching', 'tea', 'mice', 'eat', 'pig', 'piggy', 'pork', 'breakfast', 'chicken', 'rabbit', 'beef', 'bird', 'ham', 'beacon', 'spinach', 'apple', 'tiger', 'rice', 'cute', 'horse', 'cream', 'pan', 'chinchilla', 'smoothie', 'dragon', 'KFC', 'dinner', 'zoo', 'cat', 'zoology', 'dog', 'restaurant', 'broccoli', 'monkey', 'panda', 'noodle', 'bee', 'duck', 'hamster', 'elephant', 'kitten']\n",
      "Topic:  1\n",
      "['lion', 'chinchilla', 'horse', 'duck', 'mice', 'bird', 'cat', 'micky', 'rabbit', 'dragon', 'panda', 'bee', 'piggy', 'tiger', 'zoo', 'elephant', 'cute', 'hamster', 'munching', 'dog', 'pig', 'zoology', 'fish', 'kitten', 'monkey', 'vegetable', 'ozil', 'peach', 'messi', 'banana', 'win', 'lunch', 'practice', 'attack', 'defense', 'tea', 'eat', 'pork', 'breakfast', 'cup', 'chicken', 'player', 'beef', 'ham', 'shot', 'beacon', 'stadium', 'spinach', 'apple', 'rice', 'ballon', 'cream', 'hard', 'pan', 'manU', 'smoothie', 'KFC', 'dinner', 'pitch', 'dream', 'football', 'strong', 'liverpool', 'move', 'restaurant', 'fast', 'play', 'speed', 'broccoli', 'chelase', 'ronald', 'noodle']\n",
      "Topic:  2\n",
      "['cream', 'eat', 'rice', 'tea', 'smoothie', 'peach', 'noodle', 'chicken', 'vegetable', 'restaurant', 'breakfast', 'ham', 'KFC', 'spinach', 'pan', 'pork', 'apple', 'banana', 'broccoli', 'dinner', 'beef', 'lunch', 'beacon', 'shot', 'lion', 'fish', 'ozil', 'messi', 'micky', 'munching', 'win', 'practice', 'attack', 'defense', 'mice', 'pig', 'piggy', 'cup', 'rabbit', 'player', 'bird', 'stadium', 'tiger', 'cute', 'ballon', 'horse', 'hard', 'chinchilla', 'manU', 'dragon', 'pitch', 'zoo', 'cat', 'dream', 'football', 'zoology', 'strong', 'dog', 'liverpool', 'move', 'fast', 'play', 'speed', 'chelase', 'monkey', 'ronald', 'panda', 'bee', 'duck', 'hamster', 'elephant', 'kitten']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i,state in enumerate(topics_words):\n",
    "    # sorted descending word frequence within each topic\n",
    "    topic_id_freq = sorted(range(len(state)), key=lambda k: state[k], reverse=True)\n",
    "    topic_word_freq = [id2word[i] for i in topic_id_freq]\n",
    "    print(\"Topic: \", i)\n",
    "    print(topic_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., 42.,  0., 44.,  0.,  0.,  0., 41.,  1., 32., 44.,\n",
       "        37.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 36.,  0.,  0., 46.,  0.,\n",
       "         0.,  0., 35.,  0., 48.,  0.,  0.,  0.,  0.,  0., 32.,  0.,  0.,\n",
       "        44.,  0.,  0., 42.,  0.,  0.,  0.,  0., 40.,  0.,  0., 47., 29.,\n",
       "         0., 51.,  0., 43., 32.,  0., 44., 40., 52.,  0., 51.,  0., 47.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., 50., 31.,  0.,  0.,  0.,  0., 42., 37.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0., 46.,  0., 33., 40.,  0.,  0.,  0.,  0., 42.,  0.,  0.,\n",
       "        43.,  0.,  0.,  0.,  0.,  0.,  0., 40.,  0., 38.,  0., 48.,  0.,\n",
       "         0.,  0., 49.,  0.,  0., 42.,  0.,  0.,  0., 39., 43.,  0.,  0.,\n",
       "        33.,  0., 37.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 29.,  0.,\n",
       "        42.,  0., 41., 47., 38., 39., 31.],\n",
       "       [43.,  0.,  0.,  0., 45.,  0., 38.,  0.,  0.,  0., 35.,  0.,  0.,\n",
       "         0., 49.,  0., 53.,  0.,  0., 39., 41.,  0., 44.,  0.,  0., 36.,\n",
       "         0., 41.,  1., 34.,  0., 40., 39.,  0., 52.,  0.,  0.,  0., 81.,\n",
       "         0., 40.,  0.,  0., 46.,  0., 41., 37.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0., 42.,  0.,  0.,  0., 38.,  0.,  0.,  0.,\n",
       "         0., 45.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 43.] vegetable\n",
      "[ 0. 50.  0.] lion\n",
      "[ 0. 31.  0.] fish\n",
      "[42.  0.  0.] ozil\n",
      "[ 0.  0. 45.] peach\n",
      "[44.  0.  0.] messi\n",
      "[ 0.  0. 38.] banana\n",
      "[ 0. 42.  0.] micky\n",
      "[ 0. 37.  0.] munching\n",
      "[41.  0.  0.] win\n",
      "[ 1.  0. 35.] lunch\n",
      "[32.  0.  0.] practice\n",
      "[44.  0.  0.] attack\n",
      "[37.  0.  0.] defense\n",
      "[ 0.  0. 49.] tea\n",
      "[ 0. 46.  0.] mice\n",
      "[ 0.  0. 53.] eat\n",
      "[ 0. 33.  0.] pig\n",
      "[ 0. 40.  0.] piggy\n",
      "[ 0.  0. 39.] pork\n",
      "[ 0.  0. 41.] breakfast\n",
      "[36.  0.  0.] cup\n",
      "[ 0.  0. 44.] chicken\n",
      "[ 0. 42.  0.] rabbit\n",
      "[46.  0.  0.] player\n",
      "[ 0.  0. 36.] beef\n",
      "[ 0. 43.  0.] bird\n",
      "[ 0.  0. 41.] ham\n",
      "[35.  0.  1.] shot\n",
      "[ 0.  0. 34.] beacon\n",
      "[48.  0.  0.] stadium\n",
      "[ 0.  0. 40.] spinach\n",
      "[ 0.  0. 39.] apple\n",
      "[ 0. 40.  0.] tiger\n",
      "[ 0.  0. 52.] rice\n",
      "[ 0. 38.  0.] cute\n",
      "[32.  0.  0.] ballon\n",
      "[ 0. 48.  0.] horse\n",
      "[ 0.  0. 81.] cream\n",
      "[44.  0.  0.] hard\n",
      "[ 0.  0. 40.] pan\n",
      "[ 0. 49.  0.] chinchilla\n",
      "[42.  0.  0.] manU\n",
      "[ 0.  0. 46.] smoothie\n",
      "[ 0. 42.  0.] dragon\n",
      "[ 0.  0. 41.] KFC\n",
      "[ 0.  0. 37.] dinner\n",
      "[40.  0.  0.] pitch\n",
      "[ 0. 39.  0.] zoo\n",
      "[ 0. 43.  0.] cat\n",
      "[47.  0.  0.] dream\n",
      "[29.  0.  0.] football\n",
      "[ 0. 33.  0.] zoology\n",
      "[51.  0.  0.] strong\n",
      "[ 0. 37.  0.] dog\n",
      "[43.  0.  0.] liverpool\n",
      "[32.  0.  0.] move\n",
      "[ 0.  0. 42.] restaurant\n",
      "[44.  0.  0.] fast\n",
      "[40.  0.  0.] play\n",
      "[52.  0.  0.] speed\n",
      "[ 0.  0. 38.] broccoli\n",
      "[51.  0.  0.] chelase\n",
      "[ 0. 29.  0.] monkey\n",
      "[47.  0.  0.] ronald\n",
      "[ 0. 42.  0.] panda\n",
      "[ 0.  0. 45.] noodle\n",
      "[ 0. 41.  0.] bee\n",
      "[ 0. 47.  0.] duck\n",
      "[ 0. 38.  0.] hamster\n",
      "[ 0. 39.  0.] elephant\n",
      "[ 0. 31.  0.] kitten\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words_topics)):\n",
    "    print(words_topics[i],id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Justify my result with gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.049*\"lion\" + 0.048*\"chinchilla\" + 0.047*\"horse\" + 0.046*\"duck\" + 0.045*\"mice\" + 0.042*\"bird\" + 0.042*\"cat\" + 0.041*\"panda\" + 0.041*\"dragon\" + 0.041*\"micky\" + 0.041*\"rabbit\" + 0.040*\"bee\" + 0.039*\"tiger\" + 0.039*\"piggy\" + 0.038*\"elephant\" + 0.038*\"zoo\" + 0.037*\"cute\" + 0.037*\"hamster\" + 0.036*\"dog\" + 0.036*\"munching\" + 0.033*\"zoology\" + 0.033*\"pig\" + 0.031*\"fish\" + 0.031*\"kitten\" + 0.029*\"monkey\" + 0.000*\"breakfast\" + 0.000*\"strong\" + 0.000*\"rice\" + 0.000*\"chelase\" + 0.000*\"tea\" + 0.000*\"ham\" + 0.000*\"cream\" + 0.000*\"chicken\" + 0.000*\"pork\" + 0.000*\"restaurant\" + 0.000*\"dinner\" + 0.000*\"vegetable\" + 0.000*\"spinach\" + 0.000*\"broccoli\" + 0.000*\"smoothie\" + 0.000*\"fast\" + 0.000*\"banana\" + 0.000*\"hard\" + 0.000*\"speed\" + 0.000*\"win\" + 0.000*\"dream\" + 0.000*\"practice\" + 0.000*\"pan\" + 0.000*\"eat\" + 0.000*\"manU\" + 0.000*\"beacon\" + 0.000*\"pitch\" + 0.000*\"liverpool\" + 0.000*\"move\" + 0.000*\"play\" + 0.000*\"ozil\" + 0.000*\"noodle\" + 0.000*\"player\" + 0.000*\"peach\" + 0.000*\"lunch\" + 0.000*\"KFC\" + 0.000*\"attack\" + 0.000*\"defense\" + 0.000*\"apple\" + 0.000*\"ronald\" + 0.000*\"shot\" + 0.000*\"messi\" + 0.000*\"cup\" + 0.000*\"stadium\" + 0.000*\"ballon\" + 0.000*\"football\" + 0.000*\"beef\"')\n",
      "(1, '0.079*\"cream\" + 0.052*\"eat\" + 0.051*\"rice\" + 0.048*\"tea\" + 0.045*\"smoothie\" + 0.044*\"noodle\" + 0.044*\"peach\" + 0.043*\"chicken\" + 0.042*\"vegetable\" + 0.041*\"restaurant\" + 0.040*\"KFC\" + 0.040*\"ham\" + 0.040*\"breakfast\" + 0.039*\"pan\" + 0.039*\"spinach\" + 0.038*\"apple\" + 0.038*\"pork\" + 0.037*\"banana\" + 0.037*\"broccoli\" + 0.036*\"dinner\" + 0.035*\"beef\" + 0.035*\"lunch\" + 0.033*\"beacon\" + 0.000*\"manU\" + 0.000*\"fast\" + 0.000*\"hard\" + 0.000*\"attack\" + 0.000*\"defense\" + 0.000*\"stadium\" + 0.000*\"rabbit\" + 0.000*\"piggy\" + 0.000*\"ozil\" + 0.000*\"win\" + 0.000*\"speed\" + 0.000*\"dragon\" + 0.000*\"kitten\" + 0.000*\"munching\" + 0.000*\"shot\" + 0.000*\"pitch\" + 0.000*\"dog\" + 0.000*\"monkey\" + 0.000*\"strong\" + 0.000*\"dream\" + 0.000*\"chelase\" + 0.000*\"micky\" + 0.000*\"bee\" + 0.000*\"cute\" + 0.000*\"player\" + 0.000*\"horse\" + 0.000*\"messi\" + 0.000*\"move\" + 0.000*\"fish\" + 0.000*\"chinchilla\" + 0.000*\"cat\" + 0.000*\"pig\" + 0.000*\"zoology\" + 0.000*\"liverpool\" + 0.000*\"duck\" + 0.000*\"lion\" + 0.000*\"mice\" + 0.000*\"practice\" + 0.000*\"elephant\" + 0.000*\"tiger\" + 0.000*\"football\" + 0.000*\"panda\" + 0.000*\"ronald\" + 0.000*\"cup\" + 0.000*\"bird\" + 0.000*\"ballon\" + 0.000*\"hamster\" + 0.000*\"play\" + 0.000*\"zoo\"')\n",
      "(2, '0.051*\"speed\" + 0.050*\"chelase\" + 0.050*\"strong\" + 0.047*\"stadium\" + 0.046*\"ronald\" + 0.046*\"dream\" + 0.045*\"player\" + 0.043*\"messi\" + 0.043*\"attack\" + 0.043*\"hard\" + 0.043*\"fast\" + 0.042*\"liverpool\" + 0.041*\"ozil\" + 0.041*\"manU\" + 0.040*\"win\" + 0.039*\"play\" + 0.039*\"pitch\" + 0.036*\"defense\" + 0.035*\"cup\" + 0.035*\"shot\" + 0.032*\"ballon\" + 0.032*\"practice\" + 0.032*\"move\" + 0.029*\"football\" + 0.000*\"zoo\" + 0.000*\"horse\" + 0.000*\"lion\" + 0.000*\"mice\" + 0.000*\"bird\" + 0.000*\"hamster\" + 0.000*\"bee\" + 0.000*\"cat\" + 0.000*\"panda\" + 0.000*\"munching\" + 0.000*\"micky\" + 0.000*\"duck\" + 0.000*\"chinchilla\" + 0.000*\"pig\" + 0.000*\"rabbit\" + 0.000*\"piggy\" + 0.000*\"dinner\" + 0.000*\"zoology\" + 0.000*\"monkey\" + 0.000*\"dragon\" + 0.000*\"cute\" + 0.000*\"elephant\" + 0.000*\"lunch\" + 0.000*\"fish\" + 0.000*\"tea\" + 0.000*\"rice\" + 0.000*\"smoothie\" + 0.000*\"tiger\" + 0.000*\"eat\" + 0.000*\"dog\" + 0.000*\"kitten\" + 0.000*\"apple\" + 0.000*\"cream\" + 0.000*\"broccoli\" + 0.000*\"chicken\" + 0.000*\"pan\" + 0.000*\"vegetable\" + 0.000*\"banana\" + 0.000*\"pork\" + 0.000*\"beacon\" + 0.000*\"beef\" + 0.000*\"peach\" + 0.000*\"ham\" + 0.000*\"noodle\" + 0.000*\"spinach\" + 0.000*\"KFC\" + 0.000*\"restaurant\" + 0.000*\"breakfast\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "text_data = corpus\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "id_corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(id_corpus, num_topics = num_topics, id2word=dictionary, passes=12)\n",
    "#ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=num_words)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
