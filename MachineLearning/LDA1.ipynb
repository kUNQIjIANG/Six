{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "This is a demostration of LDA topic model using Gibbs sampling on a \"perfect dataset\"   \n",
    "Thanks to the clear [tutorial](https://www.cnblogs.com/pinard/p/6831308.html) provided by Pinard Liu  \n",
    "Author: kUNQI jIANG   \n",
    "Date: 2019/1/22  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus generation\n",
    "As Gibbs sampling in LDA essentially based on bag-of-words so the order of words does not matter, I use completely seperated wordset of different topic to generate pure topic documents as corpus. This is the extreme case where words and topics will be completely clustered after LDA as we can see in the result. While in real word, a word exist in different topics, and a document can cover multi-topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_set = [\"broccoli\",\"banana\",\"spinach\",\"smoothie\",\"breakfast\",\"ham\",\"cream\",\"eat\",\"vegetable\",\"dinner\",\"lunch\",\n",
    "            \"apple\",\"peach\",\"pork\",\"beef\",\"rice\",\"noodle\",\"chicken\",\"KFC\",\"restaurant\",\"cream\",\"tea\",\"pan\",\"beacon\"]\n",
    "animal_set = [\"dog\",\"cat\",\"fish\",\"chinchilla\",\"kitten\",\"cute\",\"hamster\",\"munching\",\"bird\",\"elephant\",\"monkey\",\"zoo\",\n",
    "              \"zoology\",\"pig\",\"piggy\",\"duck\",\"mice\",\"micky\",\"tiger\",\"lion\",\"horse\",\"dragon\",\"panda\",\"bee\",\"rabbit\"]\n",
    "soccer_set = [\"football\",\"pitch\",\"play\",\"player\",\"cup\",\"ballon\",\"messi\",\"ronald\",\"manU\",\"liverpool\",\"chelase\",\"ozil\",\n",
    "              \"practice\",\"hard\",\"dream\",\"stadium\",\"fast\",\"speed\",\"strong\",\"move\",\"shot\",\"attack\",\"defense\",\"win\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate(topic_set):\n",
    "    sent = np.random.choice(topic_set,10)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_set = [food_set,animal_set,soccer_set]\n",
    "corpus = []\n",
    "for i in range(100):\n",
    "    corpus.append(generate(topics_set[0]).split())\n",
    "    corpus.append(generate(topics_set[1]).split())\n",
    "    corpus.append(generate(topics_set[2]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_words = [word for document in corpus for word in document]\n",
    "vocab = set(all_words)\n",
    "num_docs = len(corpus)\n",
    "num_words = len(vocab)\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 latent topics \n",
    "num_topics = 3\n",
    "# Dirichlet prior\n",
    "alpha = np.ones([num_topics])\n",
    "#ita = term_freq\n",
    "ita = 0.1 * np.ones([num_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random assignment\n",
    "At the start randomly assign topic to each word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = []\n",
    "docs_topics = np.zeros([num_docs,num_topics]) # counts of topic assignments of each word in each doc\n",
    "words_topics = np.zeros([num_words,num_topics]) # counts of topic distributes of each word over all doc\n",
    "topics_words = np.zeros([num_topics,num_words]) # counts of word distributes of each topic over all doc\n",
    "\n",
    "for d,document in enumerate(corpus):\n",
    "    theta = np.random.dirichlet(alpha, 1)[0]\n",
    "    doc_topics = []\n",
    "    for n,word in enumerate(document):\n",
    "        sample = np.random.multinomial(1, theta, size=1)[0]\n",
    "        topic = list(sample).index(1)\n",
    "        doc_topics.append(topic)\n",
    "        docs_topics[d,topic] += 1\n",
    "        words_topics[word2id[word],topic] += 1\n",
    "        topics_words[topic,word2id[word]] += 1\n",
    "    topic_assignments.append(doc_topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita):\n",
    "    \n",
    "    topic_probs = (docs_topics[d] + alpha) / np.sum(docs_topics[d] + alpha)\n",
    "    word_sum = np.sum(topics_words + ita, axis = 1)\n",
    "    word_probs = (words_topics[word_id] + ita[word_id]) / word_sum\n",
    "    # posterior probs\n",
    "    probs = topic_probs * word_probs\n",
    "    # normalize\n",
    "    sample_probs = probs / np.sum(probs)\n",
    "    #print(sample_probs)\n",
    "    # sample new topic for current word\n",
    "    new_topic = list(np.random.multinomial(1, sample_probs, size=1)[0]).index(1)\n",
    "    return new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 9\n",
    "for j in range(num_iterations):\n",
    "    for d in range(len(corpus)):\n",
    "        document = corpus[d]\n",
    "        for n in range(len(document)):\n",
    "            word = document[n]\n",
    "            word_id = word2id[word]\n",
    "            topic = topic_assignments[d][n]\n",
    "            # exclude current word and topic\n",
    "            docs_topics[d][topic] -= 1\n",
    "            topics_words[topic][word_id] -=1\n",
    "            words_topics[word_id,topic] -= 1\n",
    "            new_topic = Gibbs_sampling(d,word_id,words_topics,docs_topics,topics_words,alpha,ita)\n",
    "            # update topic and word state\n",
    "            docs_topics[d][new_topic] += 1\n",
    "            topics_words[new_topic][word_id] += 1\n",
    "            words_topics[word_id,new_topic] += 1\n",
    "            topic_assignments[d][n] = new_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [ 0.,  0., 10.],\n",
       "       [ 0., 10.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "['cream', 'rice', 'spinach', 'dinner', 'beef', 'smoothie', 'vegetable', 'restaurant', 'chicken', 'peach', 'banana', 'tea', 'broccoli', 'beacon', 'apple', 'lunch', 'pan', 'noodle', 'pork', 'ham', 'breakfast', 'eat', 'KFC', 'messi', 'strong', 'micky', 'tiger', 'win', 'speed', 'football', 'liverpool', 'move', 'ballon', 'cat', 'fast', 'manU', 'horse', 'pig', 'lion', 'shot', 'hard', 'pitch', 'mice', 'ozil', 'hamster', 'elephant', 'chinchilla', 'zoo', 'stadium', 'panda', 'monkey', 'dragon', 'piggy', 'bird', 'zoology', 'rabbit', 'chelase', 'dog', 'fish', 'dream', 'defense', 'player', 'munching', 'kitten', 'attack', 'cute', 'cup', 'play', 'practice', 'bee', 'duck', 'ronald']\n",
      "Topic:  1\n",
      "['ozil', 'strong', 'ronald', 'liverpool', 'play', 'pitch', 'football', 'defense', 'cup', 'win', 'stadium', 'messi', 'fast', 'chelase', 'player', 'practice', 'ballon', 'shot', 'dream', 'move', 'attack', 'speed', 'manU', 'hard', 'pork', 'micky', 'tiger', 'banana', 'smoothie', 'beef', 'breakfast', 'beacon', 'rice', 'vegetable', 'eat', 'cat', 'horse', 'pig', 'lion', 'chicken', 'spinach', 'mice', 'apple', 'lunch', 'hamster', 'tea', 'cream', 'elephant', 'chinchilla', 'zoo', 'panda', 'monkey', 'dragon', 'piggy', 'KFC', 'bird', 'zoology', 'pan', 'rabbit', 'restaurant', 'dog', 'fish', 'noodle', 'dinner', 'peach', 'munching', 'ham', 'kitten', 'broccoli', 'cute', 'bee', 'duck']\n",
      "Topic:  2\n",
      "['lion', 'dragon', 'cat', 'bird', 'mice', 'zoo', 'chinchilla', 'panda', 'fish', 'kitten', 'micky', 'elephant', 'tiger', 'monkey', 'rabbit', 'munching', 'dog', 'horse', 'duck', 'cute', 'piggy', 'zoology', 'pig', 'bee', 'hamster', 'messi', 'pork', 'strong', 'banana', 'smoothie', 'beef', 'breakfast', 'win', 'speed', 'football', 'beacon', 'liverpool', 'move', 'rice', 'ballon', 'vegetable', 'eat', 'fast', 'manU', 'shot', 'chicken', 'hard', 'spinach', 'pitch', 'ozil', 'apple', 'lunch', 'tea', 'cream', 'stadium', 'KFC', 'pan', 'restaurant', 'chelase', 'dream', 'noodle', 'dinner', 'defense', 'peach', 'player', 'ham', 'broccoli', 'attack', 'cup', 'play', 'practice', 'ronald']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i,state in enumerate(topics_words):\n",
    "    # sorted descending word frequence within each topic\n",
    "    topic_id_freq = sorted(range(len(state)), key=lambda k: state[k], reverse=True)\n",
    "    topic_word_freq = [id2word[i] for i in topic_id_freq]\n",
    "    print(\"Topic: \", i)\n",
    "    print(topic_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 38.,  0.,  0.,  0., 40., 47., 51., 36.,  0.,  0.,  0., 39.,\n",
       "         0.,  0., 53.,  0., 47., 35.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        42.,  0., 52.,  0.,  0.,  0., 39., 39.,  0., 40., 78.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0., 29.,  0.,  0., 39.,  0., 46.,  0.,\n",
       "         0.,  0.,  0., 39., 52.,  0., 42.,  0.,  0., 37.,  0., 40.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [41.,  0., 54.,  0.,  0.,  0.,  0.,  0.,  0., 45., 30., 47.,  0.,\n",
       "        52., 35.,  0., 37.,  0.,  0.,  0., 39., 30.,  0.,  0.,  0., 37.,\n",
       "         0., 28.,  0., 48.,  0., 55.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0., 42.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 39.,\n",
       "         0.,  0., 37.,  0.,  0., 47.,  0., 39.,  0.,  0.,  0.,  0., 31.,\n",
       "         0., 47., 49., 38.,  0.,  0., 53.],\n",
       "       [ 0.,  0.,  0., 41., 40.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0., 47.,  0.,  0., 38., 32., 49.,  0.,\n",
       "         0.,  0.,  0.,  0., 45.,  0.,  0.,  0., 24.,  0.,  0., 41., 44.,\n",
       "        45.,  0., 42., 40., 48., 35.,  0., 46., 33.,  0., 40.,  0.,  0.,\n",
       "        39., 42.,  0.,  0.,  0.,  0.,  0.,  0., 40.,  0., 42.,  0.,  0.,\n",
       "        37.,  0.,  0.,  0., 32., 38.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 41.  0.] messi\n",
      "[38.  0.  0.] pork\n",
      "[ 0. 54.  0.] strong\n",
      "[ 0.  0. 41.] micky\n",
      "[ 0.  0. 40.] tiger\n",
      "[40.  0.  0.] banana\n",
      "[47.  0.  0.] smoothie\n",
      "[51.  0.  0.] beef\n",
      "[36.  0.  0.] breakfast\n",
      "[ 0. 45.  0.] win\n",
      "[ 0. 30.  0.] speed\n",
      "[ 0. 47.  0.] football\n",
      "[39.  0.  0.] beacon\n",
      "[ 0. 52.  0.] liverpool\n",
      "[ 0. 35.  0.] move\n",
      "[53.  0.  0.] rice\n",
      "[ 0. 37.  0.] ballon\n",
      "[47.  0.  0.] vegetable\n",
      "[35.  0.  0.] eat\n",
      "[ 0.  0. 47.] cat\n",
      "[ 0. 39.  0.] fast\n",
      "[ 0. 30.  0.] manU\n",
      "[ 0.  0. 38.] horse\n",
      "[ 0.  0. 32.] pig\n",
      "[ 0.  0. 49.] lion\n",
      "[ 0. 37.  0.] shot\n",
      "[42.  0.  0.] chicken\n",
      "[ 0. 28.  0.] hard\n",
      "[52.  0.  0.] spinach\n",
      "[ 0. 48.  0.] pitch\n",
      "[ 0.  0. 45.] mice\n",
      "[ 0. 55.  0.] ozil\n",
      "[39.  0.  0.] apple\n",
      "[39.  0.  0.] lunch\n",
      "[ 0.  0. 24.] hamster\n",
      "[40.  0.  0.] tea\n",
      "[78.  0.  0.] cream\n",
      "[ 0.  0. 41.] elephant\n",
      "[ 0.  0. 44.] chinchilla\n",
      "[ 0.  0. 45.] zoo\n",
      "[ 0. 42.  0.] stadium\n",
      "[ 0.  0. 42.] panda\n",
      "[ 0.  0. 40.] monkey\n",
      "[ 0.  0. 48.] dragon\n",
      "[ 0.  0. 35.] piggy\n",
      "[29.  0.  0.] KFC\n",
      "[ 0.  0. 46.] bird\n",
      "[ 0.  0. 33.] zoology\n",
      "[39.  0.  0.] pan\n",
      "[ 0.  0. 40.] rabbit\n",
      "[46.  0.  0.] restaurant\n",
      "[ 0. 39.  0.] chelase\n",
      "[ 0.  0. 39.] dog\n",
      "[ 0.  0. 42.] fish\n",
      "[ 0. 37.  0.] dream\n",
      "[39.  0.  0.] noodle\n",
      "[52.  0.  0.] dinner\n",
      "[ 0. 47.  0.] defense\n",
      "[42.  0.  0.] peach\n",
      "[ 0. 39.  0.] player\n",
      "[ 0.  0. 40.] munching\n",
      "[37.  0.  0.] ham\n",
      "[ 0.  0. 42.] kitten\n",
      "[40.  0.  0.] broccoli\n",
      "[ 0. 31.  0.] attack\n",
      "[ 0.  0. 37.] cute\n",
      "[ 0. 47.  0.] cup\n",
      "[ 0. 49.  0.] play\n",
      "[ 0. 38.  0.] practice\n",
      "[ 0.  0. 32.] bee\n",
      "[ 0.  0. 38.] duck\n",
      "[ 0. 53.  0.] ronald\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words_topics)):\n",
    "    print(words_topics[i],id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Justify my result with gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.076*\"cream\" + 0.051*\"rice\" + 0.050*\"dinner\" + 0.050*\"spinach\" + 0.049*\"beef\" + 0.046*\"vegetable\" + 0.046*\"smoothie\" + 0.045*\"restaurant\" + 0.041*\"chicken\" + 0.041*\"peach\" + 0.039*\"tea\" + 0.039*\"broccoli\" + 0.039*\"banana\" + 0.038*\"lunch\" + 0.038*\"pan\" + 0.038*\"beacon\" + 0.038*\"noodle\" + 0.038*\"apple\" + 0.037*\"pork\" + 0.036*\"ham\" + 0.035*\"breakfast\" + 0.034*\"eat\" + 0.028*\"KFC\" + 0.001*\"rabbit\" + 0.001*\"bird\" + 0.001*\"bee\" + 0.001*\"dragon\" + 0.001*\"pig\" + 0.001*\"monkey\" + 0.001*\"kitten\" + 0.001*\"elephant\" + 0.001*\"duck\" + 0.001*\"chinchilla\" + 0.001*\"lion\" + 0.001*\"cat\" + 0.001*\"zoo\" + 0.001*\"tiger\" + 0.001*\"dog\" + 0.001*\"panda\" + 0.001*\"munching\" + 0.001*\"piggy\" + 0.001*\"mice\" + 0.001*\"fish\" + 0.001*\"hamster\" + 0.001*\"zoology\" + 0.001*\"micky\" + 0.001*\"cute\" + 0.000*\"horse\" + 0.000*\"ronald\" + 0.000*\"stadium\" + 0.000*\"football\" + 0.000*\"attack\" + 0.000*\"pitch\" + 0.000*\"shot\" + 0.000*\"messi\" + 0.000*\"defense\" + 0.000*\"play\" + 0.000*\"speed\" + 0.000*\"ballon\" + 0.000*\"cup\" + 0.000*\"fast\" + 0.000*\"liverpool\" + 0.000*\"chelase\" + 0.000*\"player\" + 0.000*\"manU\" + 0.000*\"move\" + 0.000*\"ozil\" + 0.000*\"strong\" + 0.000*\"hard\" + 0.000*\"practice\" + 0.000*\"win\" + 0.000*\"dream\"')\n",
      "(1, '0.048*\"lion\" + 0.047*\"dragon\" + 0.046*\"cat\" + 0.045*\"bird\" + 0.044*\"mice\" + 0.044*\"zoo\" + 0.043*\"chinchilla\" + 0.041*\"fish\" + 0.041*\"panda\" + 0.041*\"kitten\" + 0.040*\"micky\" + 0.040*\"elephant\" + 0.039*\"munching\" + 0.039*\"tiger\" + 0.039*\"monkey\" + 0.039*\"rabbit\" + 0.038*\"dog\" + 0.038*\"horse\" + 0.037*\"duck\" + 0.037*\"cute\" + 0.034*\"piggy\" + 0.033*\"zoology\" + 0.031*\"pig\" + 0.031*\"bee\" + 0.024*\"hamster\" + 0.001*\"liverpool\" + 0.001*\"ronald\" + 0.001*\"ozil\" + 0.001*\"pitch\" + 0.001*\"banana\" + 0.001*\"peach\" + 0.001*\"dream\" + 0.000*\"strong\" + 0.000*\"play\" + 0.000*\"cup\" + 0.000*\"win\" + 0.000*\"ham\" + 0.000*\"apple\" + 0.000*\"beef\" + 0.000*\"cream\" + 0.000*\"move\" + 0.000*\"ballon\" + 0.000*\"speed\" + 0.000*\"noodle\" + 0.000*\"messi\" + 0.000*\"practice\" + 0.000*\"manU\" + 0.000*\"spinach\" + 0.000*\"fast\" + 0.000*\"restaurant\" + 0.000*\"attack\" + 0.000*\"defense\" + 0.000*\"chelase\" + 0.000*\"beacon\" + 0.000*\"stadium\" + 0.000*\"dinner\" + 0.000*\"rice\" + 0.000*\"broccoli\" + 0.000*\"smoothie\" + 0.000*\"pan\" + 0.000*\"pork\" + 0.000*\"eat\" + 0.000*\"player\" + 0.000*\"vegetable\" + 0.000*\"breakfast\" + 0.000*\"tea\" + 0.000*\"lunch\" + 0.000*\"shot\" + 0.000*\"KFC\" + 0.000*\"football\" + 0.000*\"chicken\" + 0.000*\"hard\"')\n",
      "(2, '0.054*\"ozil\" + 0.053*\"strong\" + 0.052*\"ronald\" + 0.051*\"liverpool\" + 0.048*\"play\" + 0.047*\"pitch\" + 0.046*\"football\" + 0.046*\"defense\" + 0.046*\"cup\" + 0.044*\"win\" + 0.041*\"stadium\" + 0.040*\"messi\" + 0.038*\"player\" + 0.038*\"chelase\" + 0.038*\"fast\" + 0.037*\"practice\" + 0.036*\"shot\" + 0.036*\"ballon\" + 0.036*\"dream\" + 0.034*\"move\" + 0.030*\"attack\" + 0.030*\"manU\" + 0.029*\"speed\" + 0.028*\"hard\" + 0.001*\"fish\" + 0.001*\"bird\" + 0.001*\"panda\" + 0.001*\"elephant\" + 0.000*\"micky\" + 0.000*\"mice\" + 0.000*\"tiger\" + 0.000*\"zoo\" + 0.000*\"monkey\" + 0.000*\"rabbit\" + 0.000*\"horse\" + 0.000*\"zoology\" + 0.000*\"kitten\" + 0.000*\"cat\" + 0.000*\"cute\" + 0.000*\"duck\" + 0.000*\"hamster\" + 0.000*\"chinchilla\" + 0.000*\"pig\" + 0.000*\"munching\" + 0.000*\"piggy\" + 0.000*\"bee\" + 0.000*\"dog\" + 0.000*\"breakfast\" + 0.000*\"noodle\" + 0.000*\"dragon\" + 0.000*\"peach\" + 0.000*\"dinner\" + 0.000*\"spinach\" + 0.000*\"vegetable\" + 0.000*\"ham\" + 0.000*\"beacon\" + 0.000*\"cream\" + 0.000*\"beef\" + 0.000*\"smoothie\" + 0.000*\"lion\" + 0.000*\"restaurant\" + 0.000*\"tea\" + 0.000*\"apple\" + 0.000*\"KFC\" + 0.000*\"eat\" + 0.000*\"chicken\" + 0.000*\"rice\" + 0.000*\"lunch\" + 0.000*\"pan\" + 0.000*\"broccoli\" + 0.000*\"banana\" + 0.000*\"pork\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "text_data = corpus\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "id_corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(id_corpus, num_topics = num_topics, id2word=dictionary, passes=12)\n",
    "#ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=num_words)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
